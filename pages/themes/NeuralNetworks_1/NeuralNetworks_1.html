<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>NeuralNetworks_1</title>
	<link rel="shortcut icon" href="/ML_SA-Slides/favicon.ico">
	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
	<!-- css & themes include -->
	<link rel="stylesheet" href="/ML_SA-Slides/lib/reveal.js/css/reveal.css">
	<link rel="stylesheet" href="/ML_SA-Slides/outfit/css/themes/projector.css" id="theme">
	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement( 'link' );
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match( /print-pdf/gi ) ? '/ML_SA-Slides/lib/reveal.js/css/print/pdf.css' : '/ML_SA-Slides/lib/reveal.js/css/print/paper.css';
		document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>
	<!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
	<!-- CUSTOM -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
	<base target="_blank">
</head>
<body>
	<div class="reveal default center" data-transition-speed="default" data-background-transition="default">
	<div class="top_links">
		<a class="home_link" href="/ML_SA-Slides/pages/agenda/agenda.html#NeuralNetworks_1" target="_top"></a>
		<span class="help_link"><i class="fa-solid fa-circle-question"></i></span>
		<div class="help_text">
			<div class="note">Keyboard shortcuts:</div>
			<div><span>N/Спейс</span><span>Next Slide</span></div>
			<div><span>P</span><span>Previous Slide</span></div>
			<div><span>O</span><span>Slides Overview</span></div>
			<div><span>ctrl+left click</span><span>Zoom Element</span></div>
			<div class="print-howto"><br>If you want print version => add '<code>?print-pdf</code>' <br> at the end of slides URL (remove '#' fragment) and then print. <br>
			Like: https://wwwcourses.github.io/...CourseIntro.html?print-pdf </div>
		</div>
	</div>
	<div class="footer theme_switch">
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/dark.css'); return false;">Dark</a>
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/light.css'); return false;">Light</a>
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/projector.css'); return false;">Projector</a>
	</div>
	<div class="slides">
<!--
########################################################
##################### SLIDES START #####################
########################################################
-->
<section data-min-total="__X__" class="presentation-title"><h1>Introduction to Artificial Neural Networks (ANN)</h1></section>
<section data-transition="zoom">
	<section class="copyright" data-transition="zoom">
		<div>
			<p style="text-align: center;">Created for</p>
		</div>
		<div class="company">
			<a href="https://softwareacademy.bg/">
			<img style="height:80vh" src="/ML_SA-Slides/outfit/images/logos/software-web@4x.png" alt="software-web@4x.png">
			</a>
		</div>
		<div class="author">
			<span class="note"><a href="https://www.linkedin.com/in/ivapopova/">Iva E. Popova</a>,  2022,</span>
			<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
			<!-- <i class="fa fa-linkedin"></i> -->
		</div>
	</section>
</section>


<section data-min="__Y__" class="main-sesction-title"><h1>Overview</h1></section>
<section class="sub-sections"><h2>Overview</h2>
	<section>
		<dl class="fa">
			<dt>Artificial Neural Networks (ANNs) are a subfield of machine learning that is inspired by the structure and function of the human brain.</dt>
			<dt>ANNs are composed of a large number of interconnected processing units called neurons, which are organized in layers.</dt>
			<dt>ANNs are used to model complex nonlinear relationships between input and output data and to <span class="note">learn from examples without being explicitly programmed</span>.</dt>
		</dl>
		<a href="./images/DL.gif"><img src="./images/DL.gif" alt="" style="height: 50vh;"></a>
	</section>
	<section><h3>The inspiration</h3>
		<dl class="fa">
			<dt>Artificial Neural Networks (ANNs) are inspired by the structure and function of the human brain. The idea of ANNs is based on the concept that the brain is a network of interconnected neurons that communicate with each other to process and transmit information. </dt>
			<dt>The neurons in the human brain receive input signals from other neurons through dendrites, perform computations in the cell body, and then produce an output signal through an axon that connects to other neurons. The communication between neurons in the brain occurs through synapses, which are the connections between the axon of one neuron and the dendrites of another neuron.</dt>
		</dl>
	</section>
	<section><h3>Biological Neuron vs Artificial Neuron</h3>
		<a href="./images/bioneuron_vs_artneuron.png"><img src="./images/bioneuron_vs_artneuron.png" alt="bioneuron_vs_artneuron" style="height: 50vh;"></a>
		<table>
			<tr>
				<th>Biological Neuron</th>
				<th>Artificial Neuron</th>
			</tr>
			<tr>
				<td>dendrites</td>
				<td>input</td>
			</tr>
			<tr>
				<td>soma (cell nucleus)</td>
				<td>node</td>
			</tr>
			<tr>
				<td>axon</td>
				<td>output</td>
			</tr>
			<tr>
				<td>synapses</td>
				<td>interconnections</td>
			</tr>
		</table>
	</section>
	<section><h3>The neuron</h3>
		<dl class="fa">
			<dt>The neurons in an ANN receive input signals, perform computations, and then produce an output signal.</dt>
			<dt>The input signals are usually multiplied by a set of weights, which are adjusted during the training process to minimize the error between the predicted output and the actual output. </dt>
			<dt>The weighted input signals are then passed through an activation function, which produces the output signal.</dt>
		</dl>
	</section>
</section>

<section data-min="50" class="main-sesction-title"><h1>ANN Structure</h1></section>
<section class="sub-sections"><h2>ANN Structure</h2>
	<section>
		<dl class="fa">
			<dt>The structure of an ANN can be described as a directed graph, where each neuron in one layer is connected to every neuron in the next layer. </dt>
			<dt>The input layer is where the data is fed into the network, and the output layer produces the result. </dt>
			<dt>In between, there can be one or more hidden layers, which are used to extract relevant features from the input data.</dt>
			<a href="./images/ANN_Structure.png"><img src="./images/ANN_Structure.png" alt="ANN_Structure.png"></a>
		</dl>
	</section>
	<section><h3>Layers</h3>
		<dl class="fa">
			<dt>Input layer</dt>
			<dd>This is the layer where the input data is fed into the network.</dd>
			<dd>The number of neurons in the input layer is determined by the size of the input data.</dd>
			<dd>For example, if we are using a neural network for image classification, the input layer will have neurons corresponding to the pixels of the image.</dd>
			<dt>Hidden layer</dt>
			<dd>This is the layer that lies between the input and output layers of the network.</dd>
			<dd>The number of hidden layers and the number of neurons in each hidden layer can vary depending on the complexity of the problem being solved</dd>
			<dd>The hidden layer(s) are used to extract relevant features from the input data</dd>
			<dt>Output layer</dt>
			<dd>This is the final layer of the network that produces the output. </dd>
			<dd>The number of neurons in the output layer is determined by the type of problem being solved. </dd>
			<dd>For example, in a binary classification problem, the output layer will have two neurons corresponding to the two possible classes.</dd>
		</dl>
	</section>
</section>

<section data-min="50" class="main-sesction-title"><h1>Activation Function</h1></section>
<section class="sub-sections"><h2>Activation Function</h2>
	<section>
		<dl class="fa">
			<dt>Activation functions are an essential part of Neural Networks as they are responsible for introducing non-linearity into the model. Without activation functions, Neural Networks would essentially be a series of linear operations, which would severely limit the model's capacity to learn complex patterns.</dt>
			<dt>There are several types of activation functions used in ANNs, including sigmoid, ReLU, and tanh. </dt>
			<dt>The choice of activation function depends on the problem being solved and the type of data being processed.</dt>
		</dl>
	</section>
	<section><h3>Sigmoid Function</h3>
		<dl class="fa">
			<dt>
				The Sigmoid function is one of the earliest and most widely used activation functions in Neural Networks. The Sigmoid function maps any input value to a value between 0 and 1, which makes it useful in binary classification problems. The mathematical formula for the Sigmoid function is given below:

				$$\\sigma(x) = \frac{1}{1 + e^{-x}}$$

			</dt>
			<dt>The Sigmoid function has the following properties:</dt>
			<dd>It is a smooth, continuous function.</dd>
			<dd>Its output is always between 0 and 1.</dd>
			<dd>The output is centered around 0.5, which can make it problematic in cases where the input values are very large or very small.</dd>
		</dl>
		<img src="./images/sigmoit-plot.png" alt="">
	</section>
	<section><h3>ReLU Function</h3>
		<dl class="fa">
			<dt>The Rectified Linear Unit (ReLU) function is another popular activation function used in Neural Networks. The ReLU function maps any input value to either 0 or the input value itself, which makes it useful in cases where we want to introduce sparsity in the model. The mathematical formula for the ReLU function is given below:

				$$ReLU(x) = max(0, x)$$</dt>
			<dt>The ReLU function has the following properties:</dt>
			<dd>It is a simple, non-linear function.</dd>
			<dd>It is computationally efficient to compute.</dd>
			<dd>The output is sparse, which can help with overfitting.</dd>
		</dl>

		<img src="./images/ReLU-plot.png" alt="">
	</section>
	<section><h3>Tanh Function</h3>
		<dl class="fa">
			<dt>The Tanh function is another popular activation function that maps any input value to a value between -1 and 1. The Tanh function is useful in cases where we want to introduce non-linearity and can help with normalization of the data. The mathematical formula for the Tanh function is given below:

				$$tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$
			</dt>
			<dt>The Tanh function has the following properties:</dt>
			<dd>It is a smooth, continuous function.</dd>
			<dd>Its output is always between -1 and 1.</dd>
			<dd>The output is centered around 0, which can help with normalization of the data.</dd>
		</dl>
		<img src="./images/Tanh-plot.png" alt="">
	</section>
	<!-- <section><h3>Softmax Function</h3>
		<dl class="fa">
			<dt>
				The Softmax function is commonly used in the output layer of Neural Networks that are designed for multi-class classification tasks. The Softmax function maps a vector of real-valued numbers to a probability distribution over the classes. The mathematical formula for the Softmax function is given below:

				$$softmax(x\_i) = \frac{e^{x\_i}}{\\sum\_{j=1}^{K} e^{x\_j}}$$

				where $K$ is the number of classes.
			</dt>
			<dt>
				The Softmax function has the following properties:</dt>
				<dd>It produces a probability distribution over the classes.</dd>
				<dd>The output values are always between 0 and 1 and sum up to 1.</dd>
		</dl>
		<img src="./images/softmax-plot.png" alt="softmax">
	</section> -->
</section>

<section data-min="50" class="main-sesction-title"><h1>ANN training</h1></section>
<section class="sub-sections"><h2>ANN training</h2>
	<section>
		<dl class="fa">
			<dt>Training an ANN involves adjusting the weights of the connections between neurons to minimize the error between the predicted output and the actual output. </dt>
			<dt>This is done using a process called backpropagation, which involves calculating the gradient of the error with respect to each weight and adjusting the weight in the direction that reduces the error.</dt>
		</dl>
	</section>
	<section><h3>Backpropagation</h3>
		<dl class="fa">
			<dt>Backpropagation is a powerful method for training neural networks. It allows us to compute the gradients of the loss function with respect to the weights of the network, which can then be used to update the weights using an optimization algorithm such as stochastic gradient descent. The key to backpropagation is the use of the chain rule of calculus to break down the gradient calculation into simpler steps.</dt>
		</dl>
	</section>
	<section><h3>Backpropagation Algorithm</h3>
		<p>Here's how it works in simple terms:</p>
		<dl class="fa">
			<dt>1. Feedforward: The input data is fed into the network, and the output is calculated by passing it through a series of interconnected nodes, also known as neurons.</dt>
			<dt>2. Calculate Error: The difference between the predicted output and the actual output is calculated, and this is known as the error.</dt>
			<dt>3. Backpropagation: The error is then propagated backwards through the network to adjust the weights and biases. The weights and biases are updated in the direction that reduces the error, using an optimization algorithm such as Stochastic Gradient Descent.</dt>
			<dt>4. Repeat: Steps 1-3 are repeated for each example in the training dataset until the error is minimized.</dt>
		</dl>
	</section>
</section>

<section data-min="50" class="main-sesction-title"><h1>Perceptron</h1></section>
<section class="sub-sections"><h2>Perceptron</h2>
	<section><h3>Overview</h3>
		<dl class="fa">
			<dt>A perceptron is a type of artificial neural network that is used for binary classification problems. It is the simplest type of neural network and consists of a single layer of neurons that are fully connected to the input data.</dt>
			<dt>The perceptron algorithm was introduced by Frank Rosenblatt in the 1950s and is based on the concept of a biological neuron.</dt>
			<dt>A single perceptron can only be used to implement linearly separable functions</dt>
		</dl>
	</section>
	<section>
		<dl class="fa">
			<a href="./images/Perceptron.png"><img src="./images/Perceptron.png" alt="Perceptron.png"></a>
			<dt>The input to a perceptron is a vector of real-valued numbers that represents the features of the input data: <code>in(t)</code></dt>
			<dt>Each feature ($x_i$) is associated with a weight ($w_i$), which is a real-valued number that determines the importance of the feature.</dt>
			<dt>The weighted sum of the inputs is then passed through an activation function, which produces the output of the perceptron.</dt>
			<dt>The activation function used in a perceptron is a step function that produces a <span class="note">binary output</span>.</dt>
		</dl>
	</section>
	<section><h3>Perceptron Activation Function</h3>
		<dl class="fa">
			<dt>If the weighted sum of the inputs is greater than a threshold value ($\theta$), the perceptron outputs a 1; otherwise, it outputs a 0:
				$$
				\text{out(t)} = \begin{cases}
				1, &\text{if } w_1x_1 + w_2x_2 + \dots + w_nx_n \geq \theta \\
				0, &\text{if } w_1x_1 + w_2x_2 + \dots + w_nx_n \lt \theta
				\end{cases}
				$$
				<br>
				where $w_1$, $w_2$, ..., $w_n$ are the weights associated with the input features $x_1$, $x_2$, ..., $x_n$.
				<br>
			</dt>
			<dt>To make things a little simpler for training, the threshold is moved to the other side of the inequality:
				$$
				\text{out(t)} = \begin{cases}
				1, &\text{if } w_1x_1 + w_2x_2 + \dots + w_nx_n + bias \geq 0 \\
				0, &\text{if } w_1x_1 + w_2x_2 + \dots + w_nx_n + bias \lt 0
				\end{cases}
				$$

				where bias = -threshold
			</dt>

		</dl>
	</section>
	<section><h3>Example: OR Function Using A Perceptron</h3>
		<dl class="fa">

		</dl>
	</section>
</section>


<section data-min="50" class="main-sesction-title"><h1>ANN Types</h1></section>
<section class="sub-sections"><h2>ANN Types</h2>
	<section><h3>Feedforward Neural Networks (FNNs)</h3>
		<dl class="fa">
			<dt>This is the simplest type of neural network, where the data flows only in one direction, from the input layer to the output layer. </dt>
			<dt>FNNs can have one or more hidden layers. </dt>
			<dt>The neurons in the input layer are connected to the neurons in the hidden layer, and the neurons in the hidden layer are connected to the neurons in the output layer.</dt>
		</dl>
	</section>
	<section><h3>Convolutional Neural Networks (CNNs)</h3>
		<dl class="fa">
			<dt>CNNs are used for image and video recognition tasks.</dt>
			<dt>They use a convolutional layer to extract relevant features from the input image, followed by a pooling layer to reduce the dimensionality of the output.</dt>
			<dt>The output of the pooling layer is then fed into a fully connected layer to produce the final output.</dt>
		</dl>
	</section>
	<section><h3>Recurrent Neural Networks (RNNs)</h3>
		<dl class="fa">
			<dt>RNNs are used for sequential data processing tasks, such as speech recognition and natural language processing. </dt>
			<dt>RNNs have a feedback loop that allows the output of a neuron to be fed back into the input of the same neuron or other neurons in the network. </dt>
			<dt>This allows RNNs to model the temporal dependencies in sequential data.</dt>
		</dl>
	</section>
	<section><h3>Long Short-Term Memory Networks (LSTMs)</h3>
		<dl class="fa">
			<dt>LSTMs are a type of RNN that are designed to address the vanishing gradient problem. </dt>
			<dt>LSTMs use memory cells that can store information for long periods of time, allowing the network to learn long-term dependencies in sequential data.</dt>
		</dl>
	</section>
</section>

<section data-min="50" class="main-sesction-title"><h1>Pros and Cons</h1></section>
<section class="sub-sections"><h2>Pros and Cons</h2>
	<section>
		<dl class="fa">
			One of the main advantages of ANNs is their ability to learn from examples and generalize to new data.
			ANNs can be used for a wide range of applications, including image and speech recognition, natural language processing, and game playing.
		</dl>
	</section>
</section>


<!-- <section data-min="1"><h1>References</h1></section>
<section><h2>References</h2>
	<section><h3>Readings</h3>
		<dl class="fa">
			<dt></dt>
		</dl>
	</section>
</section>


<section id="hw" data-min="4"><h1>Exercises</h1></section>
<section><h2>Task1: Task1Title</h2>
	<section><h3>The Task</h3>
		<dl class="fa">
			<dt></dt>
		</dl>
	</section>
</section>

<section><h3>Submission</h3>
	<dl class="fa">
		<dt>PLease, prefix your filenames/archive with your name initials, before sending.</dt>
		<dd>For instance: <b>iep_task1.py</b> or <b>iep_tasks.rar</b></dd>
		<dt>Send files to <a href="mailto:netIT.WWW.Courses@gmail.com?Subject=_NeuralNetworks_1_">netIT.WWW.Courses@gmail.com</a></dt>
	</dl>
</section> -->

<section class="disclaimer end-slide"></section>
<!--
########################################################
##################### SLIDES END   #####################
########################################################
-->
		</div>
	</div>
	<!-- Custom processing -->
	<script src="/ML_SA-Slides/outfit/js/slides.js"></script>
	<!-- external scripts -->
	<script src="/ML_SA-Slides/lib/reveal.js/lib/js/head.min.js"></script>
	<script src="/ML_SA-Slides/lib/reveal.js/js/reveal.js"></script>

	<!-- init reveal -->
	<script>
		// Full list of configuration options available at:
		// https://github.com/hakimel/reveal.js#configuration
		var highlightjsTabSize = '  ';
		Reveal.initialize({
			controls: true,
			progress: true,
			slideNumber: 'c/t',
			keyboard: true,
			history: true,

			// display control
			// center: true,
			// width: '100%',
			// height: '100%',
			// // // Factor of the display size that should remain empty around the content
			// margin: 0.1,

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1920,
			height: 1280,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0,
			maxScale: 2,

			// slide transition
			transition: 'concave', // none/fade/slide/convex/concave/zoom
			// shift+maous click to zoom in/out element
			zoomKey: 'ctrl',
			// theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
			// transition: Reveal.getQueryHash().transition || 'default'
			// Optional reveal.js plugins
			dependencies: [
				{ src: '/ML_SA-Slides/lib/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
				{ src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
				{ src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
				{ src: '/ML_SA-Slides/lib/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({tabReplace: highlightjsTabSize}); hljs.initHighlightingOnLoad(); } },
				{ src: '/ML_SA-Slides/lib/reveal.js/plugin/zoom-js/zoom.js', async: true },
				{ src: '/ML_SA-Slides/lib/reveal.js/plugin/notes/notes.js', async: true }
			]
		});
	</script>
	<!-- linkedin badge -->
	<!--<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript" async src="/ML_SA-Slides/lib/MathJax/MathJax.js?config=TeX-AMS_HTML-full"></script>
</body>
</html>
