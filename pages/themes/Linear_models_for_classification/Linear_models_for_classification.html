<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Linear_models_for_classification</title>
    <link rel="shortcut icon" href="/ML_SA-Slides/favicon.ico">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <!-- css & themes include -->
    <link rel="stylesheet" href="/ML_SA-Slides/lib/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="/ML_SA-Slides/outfit/css/themes/projector.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? '/ML_SA-Slides/lib/reveal.js/css/print/pdf.css' : '/ML_SA-Slides/lib/reveal.js/css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
    <!-- CUSTOM -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
    <base target="_blank">
</head>
<body>
    <div class="reveal default center" data-transition-speed="default" data-background-transition="default">
    <div class="top_links">
        <a class="home_link" href="/ML_SA-Slides#Linear_models_for_classification" target="_top"></a>
        <span class="help_link"><i class="fa-solid fa-circle-question"></i></span>
        <div class="help_text">
            <div class="note">Keyboard shortcuts:</div>
            <div><span>N/Спейс</span><span>Next Slide</span></div>
            <div><span>P</span><span>Previous Slide</span></div>
            <div><span>O</span><span>Slides Overview</span></div>
            <div><span>ctrl+left click</span><span>Zoom Element</span></div>
            <div class="print-howto"><br>If you want print version => add '<code>?print-pdf</code>' <br> at the end of slides URL (remove '#' fragment) and then print. <br>
            Like: https://wwwcourses.github.io/...CourseIntro.html?print-pdf </div>
        </div>
    </div>
    <div class="footer theme_switch">
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/dark.css'); return false;">Dark</a>
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/light.css'); return false;">Light</a>
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/projector.css'); return false;">Projector</a>
    </div>
    <div class="slides">
<!--
########################################################
##################### SLIDES START #####################
########################################################
-->
<section data-min-total="__X__" class="presentation-title"><h1>Linear models for classification</h1></section>
<section data-transition="zoom">
    <section class="copyright" data-transition="zoom">
        <div>
            <p style="text-align: center;">Created for</p>
        </div>
        <div class="company">
            <a href="https://softwareacademy.bg/">
            <img style="height:80vh" src="/ML_SA-Slides/outfit/images/logos/software-web@4x.png" alt="software-web@4x.png">
            </a>
        </div>
        <div class="author">
            <span class="note"><a href="https://www.linkedin.com/in/ivapopova/">Iva E. Popova</a>,  2024,</span>
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
            <!-- <i class="fa fa-linkedin"></i> -->
        </div>
    </section>
</section>



<section class="main-section-title" id="LogisticRegression"><h1>Logistic Regression</h1></section>
<section class="sub-sections"><h2>Logistic Regression</h2>
    <section><h3>Overview</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Logistic regression predicts the probability of an instance belonging to one of two classes (e.g., 0 or 1, true or false).</dt>
            <dt>Logistic regression uses the same equation as linear regression, but the sigmoid function is applied to the output of the linear equation.</dt>
            <dt>It is widely used for its simplicity and interpretability.</dt>
            <dt>Scikit: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression</a></dt>

            <a href="./images/logistic_regression_example.jpeg"><img src="./images/logistic_regression_example.jpeg" alt="./images/logistic_regression_example.jpeg" style="height: 60vh;"></a>
        </dl>
    </section>
    <section><h3>The Logistic Function</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The <a href="https://en.wikipedia.org/wiki/Logistic_function">Logistic Function</a> is a sigmoid curve that transforms any real-valued input $x$ into a value between 0 and 1. It is defined as:</dt>
            $$
            σ(x) = \frac{1}{1 + e^{-x}}

            $$
            <dt>In logistic regression, we pass to the Logistic Function the linear combination of input features and their respective coefficients (\( \theta \)):

                \[ z = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n \]

                Here, \( \theta_0, \theta_1, \dots, \theta_n \) are the model parameters (coefficients), and \( x_1, x_2, \dots, x_n \) are the input features.
                </dt>
            <dt>By passing $z$  through the sigmoid function, we obtain a value between 0 and 1, which can be interpreted as the probability of the target variable being in a certain class.</dt>
            <a href="images/sigmuid_function.png"><img src="images/sigmuid_function.png"></a>
        </dl>
    </section>
    <section>
        <h3>How it works</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The model estimates the probability \( P(Y=1|X) \) using the logistic function applied to a linear combination of input features \( X \):

                $$
                P(Y=1|X) = \sigma(\theta_0 + \theta_1 X_1 + \theta_2 X_2 + \ldots + \theta_n X_n)
                $$
             </dt>
             <dd>\( P(Y=1|X) \) is the probability that Y is equal to 1 given the input features \( X \).</dd>
             <dd>\( \theta_0, \theta_1, \ldots, \theta_n \) are the model coefficients (also called weights and often denoted by $w$).</dd>

             <dt>Logistic regression uses the log-loss (cross-entropy) cost function to measure the difference between the predicted probabilities and the actual class labels:

                $$
                J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y_i \log(h_\theta(x_i)) + (1 - y_i) \log(1 - h_\theta(x_i))]
                $$

                where \( h_\theta(x_i) \) is the predicted probability, \( y_i \) is the actual label, and \( m \) is the number of training examples.
             </dt>
             <dt>The goal is to find the weights \( \theta \) that minimize the cost function ($J(\theta)$). This is typically done using optimization algorithms like gradient descent.
            </dt>
            <dt>The model defines a decision boundary at \( P(Y=1|X) = 0.5 \). If the predicted probability is greater than 0.5, the instance is classified as class 1; otherwise, it is classified as class 0.
            </dt>
        </dl>
    </section>

    <section><h3>Example</h3>
        <dl class="fa">
            <dt>View as html: <a href="https://nbviewer.org/github/geekcourses/JupyterNotebooksExamples/blob/master/Notebooks/supervised_learning_algorihtms/linear_models_for_classification_and_regression/LogisticRegression%20-%20TitanicDemo.ipynb">LogisticRegression - TitanicDemo.html</a></dt>
        </dl>
    </section>
</section>


<section class="main-section-title"><h1>Support Vector Machines</h1></section>
<section class="sub-sections"><h2>Support Vector Machines</h2>
    <section id="SupportVectorMachines"><h3>Overview</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Support Vector Machines (SVM) are a set of supervised learning methods used for classification, regression, and outlier detection</dt>
            <dt>SVM classifier aims to find the optimal hyperplane that best separates the data into different classes. In 2D, this hyperplane is a line; in 3D, it's a plane; and in higher dimensions, it's a hyperplane.</dt>
            <dt>The hyperplane is chosen to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class. These nearest points are called support vectors.</dt>
            <dt>Scikit: <a href="https://scikit-learn.org/stable/modules/svm.html">Support Vector Machines</a></dt>
            <a href="./images/SVM_basics.png"><img src="./images/SVM_basics.png" alt="./images/SVM_basics.png"></a>
        </dl>
    </section>
    <section><h3>How it works</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>For linearly separable data, SVM finds a straight hyperplane that separates the classes. The decision function is:

                $$
                f(x) = \mathbf{w} \cdot \mathbf{x} + b
                $$

                where \( \mathbf{w} \) is the weight vector, \( \mathbf{x} \) is the input vector, and \( b \) is the bias.
             </dt>
             <dt>The SVM optimization problem aims to minimize the following cost function:

                $$
                \min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \max(0, 1 - y_i (\mathbf{w} \cdot \mathbf{x}_i + b))
                $$

                where \( C \) is the regularization parameter, \( y_i \) are the class labels, and \( \mathbf{x}_i \) are the input vectors.
             </dt>
             <dt>The parameter \( C \) controls the trade-off between maximizing the margin and minimizing the classification error.</dt>
        </dl>
    </section>
    <section><h3>Examples</h3>
        <dl class="fa" style="min-width:80vw">
            <dt><a href="https://nbviewer.org/github/geekcourses/JupyterNotebooksExamples/tree/master/Notebooks/supervised_learning_algorihtms/support_vector_machines/">Various SVM demos</a></dt>
        </dl>
    </section>
</section>

<section class="main-section-title"><h1>Perceptron</h1></section>
<section class="sub-sections"><h2>Perceptron</h2>
    <section id="Perceptron"><h3>Overview</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The perceptron is one of the simplest linear classifiers, based on a single-layer neural network.</dt>
            <dt>The Perceptron aims to find a linear decision boundary that separates two classes by updating its weights iteratively to minimize classification error.</dt>
            <dt>The Perceptron makes predictions based on a weighted sum of the input features:

                $$
                y = \begin{cases}
                1 & \text{if } \mathbf{w} \cdot \mathbf{x} + b > 0 \\
                0 & \text{otherwise}
                \end{cases}
                $$

                where \( \mathbf{w} \) is the weight vector, \( \mathbf{x} \) is the input vector, and \( b \) is the bias term.
             </dt>
            <dt>Scikit: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html">Perceptron</a></dt>
            <a href="./images/Perceptron_Classifier.png"><img src="./images/Perceptron_Classifier.png" alt="./images/Perceptron_Classifier.png"></a>
        </dl>
    </section>
</section>


<section data-min="5" class="main-section-title"><h1>Multi-class classification with Linear models.</h1></section>
<section class="sub-sections"><h2>Multi-class classification with Linear models.</h2>
    <section><h3>ne-vs-All (OvA) for Multiclass Classification</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>One-vs-All (OvA), also known as One-vs-Rest (OvR), is a strategy to extend binary classifiers to handle multiclass classification problems</dt>
        </dl>
        <a href="images/multiclass_classification2.png"><img src="images/multiclass_classification2.png"></a>
    </section>
    <section>
        <dl class="fa" style="min-width:80vw">
            <dt><b>Objective</b>: OvA aims to classify instances into one of \( K \) classes using binary classifiers.</dt>
            <dt><b>Approach</b>:</dt>
            <dd>For each class \( k \) (where \( k = 1, 2, \ldots, K \)), create a binary classifier that distinguishes between class \( k \) and all other classes.</dd>
            <dd>Train \( K \) binary classifiers. Each classifier \( C_k \) is trained to predict whether an instance belongs to class \( k \) (positive class) or not (negative class).</dd>
            <dt><b>Training</b>:</dt>
            <dd>For each classifier \( C_k \), label instances of class \( k \) as positive (\( +1 \)) and all other instances as negative (\( -1 \)).</dd>
            <dd>Train the binary classifier \( C_k \) using these labeled instances.</dd>
            <dt>Prediction</dt>
            <dd>To predict the class of a new instance, run all \( K \) binary classifiers.</dd>
            <dd>The final predicted class is the one with the highest confidence score (e.g., highest probability or margin) from its respective classifier.</dd>
        </dl>
    </section>
    <section>
        <dl class="fa" style="min-width:80vw">
            <dt><b>Advantages</b></dt>
            <dd>Simple to implement.</dd>
            <dd>Leverages existing binary classification algorithms.</dd>
            <dt><b>Disadvantages</b></dt>
            <dd>May lead to imbalanced classification problems, as each binary classifier deals with one class versus all others.</dd>
            <dd>Classifiers may be biased towards the majority class in their binary setting.</dd>
        </dl>
    </section>
</section>



<section class="disclaimer end-slide"></section>
<!--
########################################################
##################### SLIDES END   #####################
########################################################
-->
        </div>
    </div>
    <!-- Custom processing -->
    <script src="/ML_SA-Slides/outfit/js/slides.js"></script>
    <!-- external scripts -->
    <script src="/ML_SA-Slides/lib/reveal.js/lib/js/head.min.js"></script>
    <script src="/ML_SA-Slides/lib/reveal.js/js/reveal.js"></script>

    <!-- init reveal -->
    <script>
        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        var highlightjsTabSize = '  ';
        Reveal.initialize({
            controls: true,
            progress: true,
            slideNumber: 'c/t',
            keyboard: true,
            history: true,

            // display control
            // center: true,
            // width: '100%',
            // height: '100%',
            // // // Factor of the display size that should remain empty around the content
            // margin: 0.1,

            // The "normal" size of the presentation, aspect ratio will be preserved
            // when the presentation is scaled to fit different resolutions. Can be
            // specified using percentage units.
            width: 1920,
            height: 1280,

            // Factor of the display size that should remain empty around the content
            margin: 0.1,

            // Bounds for smallest/largest possible scale to apply to content
            minScale: 0,
            maxScale: 2,

            // slide transition
            transition: 'concave', // none/fade/slide/convex/concave/zoom
            // shift+maous click to zoom in/out element
            zoomKey: 'ctrl',
            // theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
            // transition: Reveal.getQueryHash().transition || 'default'
            // Optional reveal.js plugins
            dependencies: [
                { src: '/ML_SA-Slides/lib/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({tabReplace: highlightjsTabSize}); hljs.initHighlightingOnLoad(); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/zoom-js/zoom.js', async: true },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/notes/notes.js', async: true }
            ]
        });
    </script>
    <!-- linkedin badge -->
    <!--<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async src="/ML_SA-Slides/lib/MathJax/MathJax.js?config=TeX-AMS_HTML-full"></script>
</body>
</html>