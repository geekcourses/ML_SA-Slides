<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Naive_Bayes_Classifiers</title>
    <link rel="shortcut icon" href="/ML_SA-Slides/favicon.ico">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <!-- css & themes include -->
    <link rel="stylesheet" href="/ML_SA-Slides/lib/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="/ML_SA-Slides/outfit/css/themes/projector.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? '/ML_SA-Slides/lib/reveal.js/css/print/pdf.css' : '/ML_SA-Slides/lib/reveal.js/css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
    <!-- CUSTOM -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
    <base target="_blank">

    <style>
        .mj-left-align-block .MathJax_Display{
            padding: .5em 0 .5em 1em;
            text-align: left !important;
        }
    </style>
</head>
<body>
    <div class="reveal default center" data-transition-speed="default" data-background-transition="default">
    <div class="top_links">
        <a class="home_link" href="/ML_SA-Slides/pages/agenda/agenda.html#Naive_Bayes_Classifiers" target="_top"></a>
        <span class="help_link"><i class="fa-solid fa-circle-question"></i></span>
        <div class="help_text">
            <div class="note">Keyboard shortcuts:</div>
            <div><span>N/Спейс</span><span>Next Slide</span></div>
            <div><span>P</span><span>Previous Slide</span></div>
            <div><span>O</span><span>Slides Overview</span></div>
            <div><span>ctrl+left click</span><span>Zoom Element</span></div>
            <div class="print-howto"><br>If you want print version => add '<code>?print-pdf</code>' <br> at the end of slides URL (remove '#' fragment) and then print. <br>
            Like: https://wwwcourses.github.io/...CourseIntro.html?print-pdf </div>
        </div>
    </div>
    <div class="footer theme_switch">
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/dark.css'); return false;">Dark</a>
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/light.css'); return false;">Light</a>
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/projector.css'); return false;">Projector</a>
    </div>
    <div class="slides">
<!--
########################################################
##################### SLIDES START #####################
########################################################
-->
<section data-min-total="__X__" class="presentation-title"><h1>Naive Bayes Classifiers</h1></section>
<section data-transition="zoom">
    <section class="copyright" data-transition="zoom">
        <div>
            <p style="text-align: center;">Created for</p>
        </div>
        <div class="company">
            <a href="https://softwareacademy.bg/">
            <img style="height:80vh" src="/ML_SA-Slides/outfit/images/logos/software-web@4x.png" alt="software-web@4x.png">
            </a>
        </div>
        <div class="author">
            <span class="note"><a href="https://www.linkedin.com/in/ivapopova/">Iva E. Popova</a>,  2024,</span>
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
            <!-- <i class="fa fa-linkedin"></i> -->
        </div>
    </section>
</section>


<section class="main-section-title" id="NaiveBayesClassifiers"><h1>Introduction</h1></section>
<section class="sub-sections"><h2>Introduction</h2>
    <section id="Introduction"><h3>Overview</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem.</dt>
            <dt>They <span class="note">assume independence between the features</span> of a dataset (to simplifies the computation significantly), hence the term 'naive'.</dt>
            <dt>Naive Bayes classifiers work by calculating the probability of each class given a set of features and then selecting the class with the highest probability.</dt>
        </dl>
    </section>
    <section><h3>Bayes Theorem for Classification</h3>
        <dl class="fa">
            <dt>For each class $y_j$, calculate the probability of that class, given a feature vector $ {{x_1},...,{x_n}}$. I.e ${\rm{P}}\left( {y_j|{x_1},...,{x_n}} \right) = ?$</dt>
            <dt>Choose the class with highest probability.</dt>
            <dt>A Bayes Theorem can be applied to calculate that probability:</dt>
        </dl>
        <p>$${\rm{P}}\left( {y|{x_1},...,{x_n}} \right) = \frac{{{\rm{P}}\left( {{x_1},...,{x_n}|y} \right)} {\rm{P}}\left( y \right)}{{{\rm{P}}\left( {{x_1},...,{x_n}} \right)}} $$</p>
    </section>
    <section><h3>Why <span class="note">Naive</span>?</h3>
        <dl class="fa">
            <dt>It uses the <i>naive independence assumption</i> - the probability of each feature belonging to a given class is independent of all other features.</dt>
            <!-- <dd>$P(xi|y,x1,...,xi−1,xi+1,...xn)=P(xi|y)$</dd> -->
            <dt>Using this naive independence assumption, we re-express Bayes' theorem to consider the probability of features independently: <br>
                $$ {\rm{P}}\left( {y|{x_1},...,{x_n}} \right) = \frac{{\prod\nolimits_{i = 1}^n {{\rm{P}}\left( {{x_i}|y} \right) {\rm{P}}\left( y \right)} }} {{{\rm{P}}\left( {{x_1},...,{x_n}} \right)}} $$
            </dt>
            <dt>The denominator ${{\rm{P}}\left( {{x_1},...,{x_n}} \right)}$ is constant for all classes, so we can ignore it in the calculations.</dt>
            <dt>Finally, the predicted class is evaluated as: <br>
                $$ \hat y = \arg \mathop {\max }\limits_y \prod\nolimits_{i = 1}^n {{\rm{P}}\left( {{x_i}|y} \right) {\rm{P}}\left( y \right)} $$</dt>
        </dl>
    </section>
    <section><h3>Why <span class="note">Naive</span>?</h3>
        <dl class="fa">
            <dt>It uses the <i>naive independence assumption</i> - the probability of each feature belonging to a given class is independent of all other features.</dt>
            <!-- <dd>$P(xi|y,x1,...,xi−1,xi+1,...xn)=P(xi|y)$</dd> -->
            <dt>Using this naive independence assumption, we re-express Bayes' theorem to consider the probability of features independently: <br>
                $$ {\rm{P}}\left( {y|{x_1},...,{x_n}} \right) = \frac{{\prod\nolimits_{i = 1}^n {{\rm{P}}\left( {{x_i}|y} \right) {\rm{P}}\left( y \right)} }} {{{\rm{P}}\left( {{x_1},...,{x_n}} \right)}} $$</dt>
            <dd>where:</dd>
            <dd>${\rm{P}}\left( {y|{x_1},...,{x_n}} \right)$ is the posterior probability of class $y$ given the features $x_1,...,x_n$</dd>
            <dd>${\rm{P}}\left( {x_i}|y \right)$ is the likelihood of feature $x_i$ given class $y$</dd>
            <dd>${\rm{P}}\left( y \right)$ is the prior probability of class $y$</dd>
            <dd>${\rm{P}}\left( {x_1},...,{x_n} \right)$ is the total probability of the features, which is constant across classes and can be ignored during classification.</dd>
            <dt>Finally, the predicted class is evaluated as: <br>
                $$ \hat y = \arg \mathop {\max }\limits_y \prod\nolimits_{i = 1}^n {{\rm{P}}\left( {{x_i}|y} \right) {\rm{P}}\left( y \right)} $$</dt>
            <dd>This equation finds the class $y$ that maximizes the product of the likelihood of the features and the prior probability of the class.</dd>
        </dl>
    </section>
</section>

<section class="sub-sections"><h2>Example: Spam Mail Classification</h2>
    <section id="DatasetForSpamClassification"><h3>Dataset</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Imagine we want to classify an email as "spam" or "not spam" based on certain words it contains.</dt>
            <dd>Let's say we have two features: whether the email contains the word "free" and whether it contains the word "win".</dd>
            <dt>We assume the probabilities of these words appearing in spam and non-spam emails are independent.</dt>
            <dt>Here is a simple dataset for classifying emails as "spam" or "not spam" based on key words.</dt>
            <dd>
                <table style="width:100%; border: 1px solid black;">
                    <tr>
                        <th>Email</th>
                        <th>Contains "free"</th>
                        <th>Contains "offer"</th>
                        <th>Contains "meeting"</th>
                        <th>Class (Spam/Not Spam)</th>
                    </tr>
                    <tr>
                        <td>Email 1</td>
                        <td>Yes</td>
                        <td>Yes</td>
                        <td>No</td>
                        <td>Spam</td>
                    </tr>
                    <tr>
                        <td>Email 2</td>
                        <td>No</td>
                        <td>No</td>
                        <td>Yes</td>
                        <td>Not Spam</td>
                    </tr>
                    <tr>
                        <td>Email 3</td>
                        <td>Yes</td>
                        <td>No</td>
                        <td>No</td>
                        <td>Spam</td>
                    </tr>
                    <tr>
                        <td>Email 4</td>
                        <td>No</td>
                        <td>No</td>
                        <td>Yes</td>
                        <td>Not Spam</td>
                    </tr>
                    <tr>
                        <td>Email 5</td>
                        <td>Yes</td>
                        <td>Yes</td>
                        <td>No</td>
                        <td>Spam</td>
                    </tr>
                </table>
            </dd>
        </dl>
    </section>

    <section id="FeatureProbabilitiesSpamClassification"><h3>Feature Probabilities</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>From the dataset, we calculate the following probabilities:</dt>
            <dd>
                <ul>
                    <li>${\rm{P}}(spam) = \frac{3}{5}$, ${\rm{P}}(not\ spam) = \frac{2}{5}$</li>
                    <li>${\rm{P}}(free|spam) = \frac{3}{3}$, ${\rm{P}}(free|not\ spam) = \frac{0}{2}$</li>
                    <li>${\rm{P}}(offer|spam) = \frac{2}{3}$, ${\rm{P}}(offer|not\ spam) = \frac{0}{2}$</li>
                    <li>${\rm{P}}(meeting|spam) = \frac{0}{3}$, ${\rm{P}}(meeting|not\ spam) = \frac{2}{2}$</li>
                </ul>
            </dd>
        </dl>
    </section>

    <section id="BayesTheoremApplied"><h3>Applying Bayes' Theorem</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Now, consider a new email that contains the words "free" and "offer" but not "meeting".</dt>
            <dd>We calculate the probabilities for both classes:</dd>
            <dd>
                <ul>
                    <li>${\rm{P}}(spam|free, offer, not\ meeting) \propto {\rm{P}}(free|spam) \cdot {\rm{P}}(offer|spam) \cdot {\rm{P}}(not\ meeting|spam) \cdot {\rm{P}}(spam)$</li>
                    <li>${\rm{P}}(not\ spam|free, offer, not\ meeting) \propto {\rm{P}}(free|not\ spam) \cdot {\rm{P}}(offer|not\ spam) \cdot {\rm{P}}(not\ meeting|not\ spam) \cdot {\rm{P}}(not\ spam)$</li>
                </ul>
            </dd>
        </dl>
    </section>

    <section id="CalculationOfProbabilities"><h3>Calculating the Probabilities</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>For "spam":</dt>
            <dd>
                ${\rm{P}}(spam|free, offer, not\ meeting) \propto \frac{3}{3} \cdot \frac{2}{3} \cdot \frac{3}{5} = 0.4$
            </dd>

            <dt>For "not spam":</dt>
            <dd>
                ${\rm{P}}(not\ spam|free, offer, not\ meeting) \propto \frac{0}{2} \cdot \frac{0}{2} \cdot \frac{2}{2} = 0$
            </dd>
        </dl>
    </section>

    <section id="FinalDecisionSpamClassification"><h3>Final Decision</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Since ${\rm{P}}(spam|free, offer, not\ meeting)$ is much higher than ${\rm{P}}(not\ spam|free, offer, not\ meeting)$, the email is classified as "spam".</dt>
        </dl>
    </section>
</section>


<section data-min="__Y__" class="main-section-title"><h1>Types of Naive Bayes Classifiers</h1></section>
<section class="sub-sections"><h2>Overview</h2>
    <section>
        <dl class="fa">
            <p>Another assumption made is about the probability distribution of the input features. Depending of that, there are several types of Naive Bayes Classifier</p>
            <dt>Bernoulli Naive Bayes Classifier</dt>
            <dd>The features are discrete Boolean data (1 or 0) and <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a> is used</dd>
            <dt>Multinomial Naive Bayes Classifier</dt>
            <dd>Features are discrete (usually representing a count), and <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial distribution</a> is used</dd>
            <dt>Gaussian Naive Bayes Classifier</dt>
            <dd>Features are continuous and the <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal ( Gaussian) distribution</a> is used</dd>
        </dl>
    </section>
</section>
<section class="sub-sections"><h2>Bernoulli naive Bayes.</h2>
    <section><h3>Overview</h3>
        <dl class="fa">
            <dt>Bernoulli naive Bayes Classifier is applicable for binary features.</dt>
            <dt>I.e. when the features probability distribution is a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a></dt>
            <dt>It finds application in text classification, where the input vectors represent the presence (1) or absence (0) of a word from the lexicon in a document.</dt>
        </dl>
    </section>
</section>
<section class="sub-sections"><h2>Gaussian Naive Bayes.</h2>
    <section>
        <dl class="fa">
            <dt>Description and examples are given in the notebook: <a href="examples/slides/NaiveBayes.html">NaiveBayes.html</a></dt>
        </dl>
    </section>
</section>
<section class="sub-sections"><h2>Multinomial naive Bayes.</h2>
    <section>
        <dl class="fa">
            <dt>Description and examples are given in the notebook: <a href="examples/slides/MultinomialNaiveBayes.html">MultinomialNaiveBayes.html</a></dt>
        </dl>
    </section>
</section>

<section class="main-section-title"><h1>Applications</h1></section>
<section class="sub-sections"><h2>Applications</h2>
    <section id="Applications"><h3>Applications</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Naive Bayes classifiers are commonly used in spam filtering, text classification, and sentiment analysis.</dt>
            <dd>Their simplicity makes them easy to implement and useful for large datasets.</dd>
        </dl>
    </section>
</section>


<section data-min="__Y__" class="main-section-title"><h1>Pros and cons of Naive Bayes classifiers.</h1></section>
<section class="sub-sections"><h2>Pros and cons of Naive Bayes classifiers.</h2>
    <section>
        <dl class="fa">
            <dt>Pros:</dt>
            <dd>Simple and fast to implement.</dd>
            <dd>Performs well with large datasets.</dd>
            <dd>Effective for text classification problems like spam detection and sentiment analysis.</dd>
            <dd>Works well even with a small amount of training data.</dd>
            <dd>Requires less computational power compared to other algorithms.</dd>

            <dt>Cons:</dt>
            <dd>Assumes independence between features, which is often unrealistic in real-world data.</dd>
            <dd>Can perform poorly with highly correlated features.</dd>
            <dd>Limited in complex applications where relationships between features are important.</dd>
        </dl>
    </section>
    <section><h3>When to use?</h3>
        <p>Naive Bayes classifiers tend to perform especially well in one of the following situations:</p>
        <dl class="fa">
            <dt>When the naive assumptions actually match the data (very rare in practice)</dt>
            <dt>For very well-separated categories, when model complexity is less important</dt>
            <dt>For very high-dimensional data, when model complexity is less important</dt>
        </dl>
    </section>
</section>



<section id="hw" data-min="4"><h1>Exercises</h1></section>
<section class="sub-sections"><h2>Task1: Adult/Child classification
</h2>
    <section><h3>The Task</h3>
        <dl class="fa">
            <dt>The task is given in next JupyterNotebook: <a href="https://github.com/geekcourses/JupyterNotebooksExamples/blob/master/Notebooks/NaiveBayes/naive_bayes_children_and_adults_sklearn_TASK.ipynb">naive_bayes_children_and_adults_sklearn_TASK.ipynb</a></dt>
        </dl>
    </section>
</section>

<section><h3>Submission</h3>
    <dl class="fa">
        <dt>PLease, prefix your filenames/archive with your name initials, before sending.</dt>
        <dd>For instance: <b>iep_task1.py</b> or <b>iep_tasks.rar</b></dd>
        <dt>Send files to <a href="mailto:netIT.WWW.Courses@gmail.com?Subject=_Naive_Bayes_Classifiers_">netIT.WWW.Courses@gmail.com</a></dt>
    </dl>
</section>

<section class="disclaimer end-slide"></section>
<!--
########################################################
##################### SLIDES END   #####################
########################################################
-->
        </div>
    </div>
    <!-- Custom processing -->
    <script src="/ML_SA-Slides/outfit/js/slides.js"></script>
    <!-- external scripts -->
    <script src="/ML_SA-Slides/lib/reveal.js/lib/js/head.min.js"></script>
    <script src="/ML_SA-Slides/lib/reveal.js/js/reveal.js"></script>

    <!-- init reveal -->
    <script>
        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        var highlightjsTabSize = '  ';
        Reveal.initialize({
            controls: true,
            progress: true,
            slideNumber: 'c/t',
            keyboard: true,
            history: true,

            // display control
            // center: true,
            // width: '100%',
            // height: '100%',
            // // // Factor of the display size that should remain empty around the content
            // margin: 0.1,

            // The "normal" size of the presentation, aspect ratio will be preserved
            // when the presentation is scaled to fit different resolutions. Can be
            // specified using percentage units.
            width: 1920,
            height: 1280,

            // Factor of the display size that should remain empty around the content
            margin: 0.1,

            // Bounds for smallest/largest possible scale to apply to content
            minScale: 0,
            maxScale: 2,

            // slide transition
            transition: 'concave', // none/fade/slide/convex/concave/zoom
            // shift+maous click to zoom in/out element
            zoomKey: 'ctrl',
            // theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
            // transition: Reveal.getQueryHash().transition || 'default'
            // Optional reveal.js plugins
            dependencies: [
                { src: '/ML_SA-Slides/lib/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({tabReplace: highlightjsTabSize}); hljs.initHighlightingOnLoad(); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/zoom-js/zoom.js', async: true },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/notes/notes.js', async: true }
            ]
        });
    </script>
    <!-- linkedin badge -->
    <!--<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async src="/ML_SA-Slides/lib/MathJax/MathJax.js?config=TeX-AMS_HTML-full"></script>
</body>
</html>
