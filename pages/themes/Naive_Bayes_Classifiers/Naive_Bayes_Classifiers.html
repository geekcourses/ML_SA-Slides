<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Naive_Bayes_Classifiers</title>
    <link rel="shortcut icon" href="/ML_SA-Slides/favicon.ico">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <!-- css & themes include -->
    <link rel="stylesheet" href="/ML_SA-Slides/lib/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="/ML_SA-Slides/outfit/css/themes/projector.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? '/ML_SA-Slides/lib/reveal.js/css/print/pdf.css' : '/ML_SA-Slides/lib/reveal.js/css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
    <!-- CUSTOM -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
    <base target="_blank">

    <style>
        .mj-left-align-block .MathJax_Display{
            padding: .5em 0 .5em 1em;
            text-align: left !important;
        }
    </style>
</head>
<body>
    <div class="reveal default center" data-transition-speed="default" data-background-transition="default">
    <div class="top_links">
        <a class="home_link" href="/ML_SA-Slides/pages/agenda/agenda.html#Naive_Bayes_Classifiers" target="_top"></a>
        <span class="help_link"><i class="fa-solid fa-circle-question"></i></span>
        <div class="help_text">
            <div class="note">Keyboard shortcuts:</div>
            <div><span>N/Спейс</span><span>Next Slide</span></div>
            <div><span>P</span><span>Previous Slide</span></div>
            <div><span>O</span><span>Slides Overview</span></div>
            <div><span>ctrl+left click</span><span>Zoom Element</span></div>
            <div class="print-howto"><br>If you want print version => add '<code>?print-pdf</code>' <br> at the end of slides URL (remove '#' fragment) and then print. <br>
            Like: https://wwwcourses.github.io/...CourseIntro.html?print-pdf </div>
        </div>
    </div>
    <div class="footer theme_switch">
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/dark.css'); return false;">Dark</a>
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/light.css'); return false;">Light</a>
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/projector.css'); return false;">Projector</a>
    </div>
    <div class="slides">
<!--
########################################################
##################### SLIDES START #####################
########################################################
-->
<section data-min-total="__X__" class="presentation-title"><h1>Naive Bayes Classifiers</h1></section>
<section data-transition="zoom">
    <section class="copyright" data-transition="zoom">
        <div>
            <p style="text-align: center;">Created for</p>
        </div>
        <div class="company">
            <a href="https://softwareacademy.bg/">
            <img style="height:80vh" src="/ML_SA-Slides/outfit/images/logos/software-web@4x.png" alt="software-web@4x.png">
            </a>
        </div>
        <div class="author">
            <span class="note"><a href="https://www.linkedin.com/in/ivapopova/">Iva E. Popova</a>,  2024,</span>
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
            <!-- <i class="fa fa-linkedin"></i> -->
        </div>
    </section>
</section>


<section class="main-section-title" id="NaiveBayesClassifiers"><h1>Introduction</h1></section>
<section class="sub-sections"><h2>Introduction</h2>
    <section id="Introduction"><h3>Overview</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem.</dt>
            <dt>They <span class="note">assume independence between the features</span> of a dataset (to simplifies the computation significantly), hence the term 'naive'.</dt>
            <dt>Naive Bayes classifiers work by calculating the probability of each class given a set of features and then selecting the class with the highest probability.</dt>
        </dl>
    </section>
    <section><h3>Bayes Theorem for Classification</h3>
        <dl class="fa">
            <dt>For each class $y_j$, calculate the probability of that class, given a feature vector $ {{x_1},...,{x_n}}$. I.e ${\rm{P}}\left( {y_j|{x_1},...,{x_n}} \right) = ?$</dt>
            <dt>Choose the class with highest probability.</dt>
            <dt>A Bayes Theorem can be applied to calculate that probability:</dt>
        </dl>
        <p>$${\rm{P}}\left( {y|{x_1},...,{x_n}} \right) = \frac{{{\rm{P}}\left( {{x_1},...,{x_n}|y} \right)} {\rm{P}}\left( y \right)}{{{\rm{P}}\left( {{x_1},...,{x_n}} \right)}} $$</p>
    </section>
    <!-- <section><h3>Why <span class="note">Naive</span>?</h3>
        <dl class="fa">
            <dt>It uses the <i>naive independence assumption</i> - the probability of each feature belonging to a given class is independent of all other features.</dt>
            <dd>$P(xi|y,x1,...,xi−1,xi+1,...xn)=P(xi|y)$</dd>
            <dt>Using this naive independence assumption, we re-express Bayes' theorem to consider the probability of features independently: <br>
                $$ {\rm{P}}\left( {y|{x_1},...,{x_n}} \right) = \frac{{\prod\nolimits_{i = 1}^n {{\rm{P}}\left( {{x_i}|y} \right) {\rm{P}}\left( y \right)} }} {{{\rm{P}}\left( {{x_1},...,{x_n}} \right)}} $$
            </dt>
            <dt>The denominator ${{\rm{P}}\left( {{x_1},...,{x_n}} \right)}$ is constant for all classes, so we can ignore it in the calculations.</dt>
            <dt>Finally, the predicted class is evaluated as: <br>
                $$ \hat y = \arg \mathop {\max }\limits_y \prod\nolimits_{i = 1}^n {{\rm{P}}\left( {{x_i}|y} \right) {\rm{P}}\left( y \right)} $$</dt>
        </dl>
    </section> -->
    <section><h3>Why <span class="note">Naive</span>?</h3>
        <dl class="fa">
            <dt>The classifier is called "naive" because it assumes that the features are conditionally independent given the class. This is known as the <i>naive independence assumption</i>.</dt>
            <dt>Using this assumption, we can express the joint likelihood of the features as the product of individual likelihoods. Therefore, Bayes' theorem for this model is simplified to:</dt>

                $$ \color{blue}{P(y|{x_1},...,{x_n})} = \frac{\prod_{i=1}^{n} \color{red}{P(x_i|y)} \cdot \color{green}{P(y)}}{\color{orange}{P(x_1, ..., x_n)}} $$

            <dd>where:</dd>
            <dd>\(\color{blue}{P(y|{x_1},...,{x_n})}\) is the <span style="color:blue">posterior</span> probability of class \(y\) given the features \(x_1,...,x_n\).</dd>
            <dd>\(\color{red}{P(x_i|y)}\) is the <span style="color: red;">likelihood</span> of feature \(x_i\) given class \(y\), and the product arises because of the naive independence assumption.</dd>
            <dd>\(\color{green}{P(y)}\) is the <span style="color: green;">prior</span> probability of class \(y\).</dd>
            <dd>\(\color{orange}{P(x_1,...,x_n)}\) is the total (<span style="color: orange;">marginal</span> ) probability of the features, which is constant across all classes. Thus:</dd>

                $$ \color{blue}{P(y|{x_1},...,{x_n})} \propto \prod_{i=1}^{n} \color{red}{P(x_i|y)} \cdot \color{green}{P(y)} $$
            <dt>The predicted class, \(\hat y\), is the one that maximizes the following expression:</dt>

                $$ \hat{y} = \arg \max_y \prod_{i=1}^{n} \color{red}{P(x_i|y)} \cdot \color{green}{P(y)} $$

            <dd>This equation finds the class $y$ with the highest product of likelihoods for the features and the prior probability of the class.</dd>
        </dl>
    </section>
</section>


<section class="main-section-title" id="NaiveBayesClassifiers"><h1>Example: Spam Mail Classification</h1></section>
<section class="sub-sections"><h2>Example: Spam Mail Classification</h2>
    <section><h3></h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Example is given in next Jupyter Notebook: <a href="https://nbviewer.org/github/geekcourses/JupyterNotebooksExamples/blob/master/Notebooks/supervised_learning_algorihtms/NaiveBayes/naive_bayes_classifier_explained_by_simple_example_spam_ham.ipynb">Naive Bayes Classifier explained by simple example (Spam/Ham Mails)</a></dt>
        </dl>
    </section>
</section>


<section data-min="__Y__" class="main-section-title"><h1>Types of Naive Bayes Classifiers</h1></section>
<section class="sub-sections"><h2>Overview</h2>
    <section>
        <h3>Overview</h3>
        <dl class="fa">
            <dt>Naive Bayes classifiers also make assumptions about the probability distribution of the input features. Depending on the type of data, different distributions are used, leading to several types of Naive Bayes classifiers:</dt>

            <dd><b>Bernoulli Naive Bayes Classifier</b> - This is used when the features are binary (yes/no or 1/0). For example, whether an email contains the word "free" (yes or no). It assumes each feature follows a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a>, meaning the feature is either present or absent.</dd>

            <dd><b>Multinomial Naive Bayes Classifier</b> - This is used when the features are counts, such as the number of times a word appears in a document or the number of products purchased. It assumes the features follow a <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial distribution</a>, which models the occurrence of events in different categories.</dd>

            <dd><b>Gaussian Naive Bayes Classifier</b> - This is used when the features are continuous, like the price of a product or the temperature in a weather dataset. It assumes the features follow a <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian (normal) distribution</a>, which is the bell-curve distribution often seen with real-world measurements.</dd>
        </dl>
    </section>
    <section><h3>References</h3>
        <dl class="fa" style="min-width:80vw">
            <dt><a href="examples/slides/GaussianNaiveBayes.html">Gaussian Naive Bayes - Example: Classifying Text</a></dt>
            <dt><a href="examples/slides/MultinomialNaiveBayes.html">Multinomial Naive Bayes - Example: Classifying Text</a></dt></dt>
        </dl>
    </section>
</section>

<section class="main-section-title"><h1>Applications</h1></section>
<section class="sub-sections"><h2>Applications</h2>
    <section id="Applications"><h3>Applications</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Naive Bayes classifiers are commonly used in spam filtering, text classification, and sentiment analysis.</dt>
            <dt>Their simplicity makes them easy to implement and useful for large datasets.</dt>
        </dl>
    </section>
</section>


<section data-min="__Y__" class="main-section-title"><h1>Pros and cons of Naive Bayes classifiers.</h1></section>
<section class="sub-sections"><h2>Pros and cons of Naive Bayes classifiers.</h2>
    <section>
        <dl class="fa">
            <dt>Pros:</dt>
            <dd>Simple and fast to implement.</dd>
            <dd>Performs well with large datasets.</dd>
            <dd>Effective for text classification problems like spam detection and sentiment analysis.</dd>
            <dd>Works well even with a small amount of training data.</dd>
            <dd>Requires less computational power compared to other algorithms.</dd>

            <dt>Cons:</dt>
            <dd>Assumes independence between features, which is often unrealistic in real-world data.</dd>
            <dd>Can perform poorly with highly correlated features.</dd>
            <dd>Limited in complex applications where relationships between features are important.</dd>
        </dl>
    </section>
    <section><h3>When to use?</h3>
        <p>Naive Bayes classifiers tend to perform especially well in one of the following situations:</p>
        <dl class="fa">
            <dt>When the naive assumptions actually match the data (very rare in practice)</dt>
            <dt>For very well-separated categories, when model complexity is less important</dt>
            <dt>For very high-dimensional data, when model complexity is less important</dt>
        </dl>
    </section>
</section>

<!--

<section id="hw" data-min="4"><h1>Exercises</h1></section>
<section class="sub-sections"><h2>Task1: Adult/Child classification
</h2>
    <section><h3>The Task</h3>
        <dl class="fa">
            <dt>The task is given in next JupyterNotebook: <a href="https://github.com/geekcourses/JupyterNotebooksExamples/blob/master/Notebooks/NaiveBayes/naive_bayes_children_and_adults_sklearn_TASK.ipynb">naive_bayes_children_and_adults_sklearn_TASK.ipynb</a></dt>
        </dl>
    </section>
</section>

<section><h3>Submission</h3>
    <dl class="fa">
        <dt>PLease, prefix your filenames/archive with your name initials, before sending.</dt>
        <dd>For instance: <b>iep_task1.py</b> or <b>iep_tasks.rar</b></dd>
        <dt>Send files to <a href="mailto:netIT.WWW.Courses@gmail.com?Subject=_Naive_Bayes_Classifiers_">netIT.WWW.Courses@gmail.com</a></dt>
    </dl>
</section> -->

<section class="disclaimer end-slide"></section>
<!--
########################################################
##################### SLIDES END   #####################
########################################################
-->
        </div>
    </div>
    <!-- Custom processing -->
    <script src="/ML_SA-Slides/outfit/js/slides.js"></script>
    <!-- external scripts -->
    <script src="/ML_SA-Slides/lib/reveal.js/lib/js/head.min.js"></script>
    <script src="/ML_SA-Slides/lib/reveal.js/js/reveal.js"></script>

    <!-- init reveal -->
    <script>
        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        var highlightjsTabSize = '  ';
        Reveal.initialize({
            controls: true,
            progress: true,
            slideNumber: 'c/t',
            keyboard: true,
            history: true,

            // display control
            // center: true,
            // width: '100%',
            // height: '100%',
            // // // Factor of the display size that should remain empty around the content
            // margin: 0.1,

            // The "normal" size of the presentation, aspect ratio will be preserved
            // when the presentation is scaled to fit different resolutions. Can be
            // specified using percentage units.
            width: 1920,
            height: 1280,

            // Factor of the display size that should remain empty around the content
            margin: 0.1,

            // Bounds for smallest/largest possible scale to apply to content
            minScale: 0,
            maxScale: 2,

            // slide transition
            transition: 'concave', // none/fade/slide/convex/concave/zoom
            // shift+maous click to zoom in/out element
            zoomKey: 'ctrl',
            // theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
            // transition: Reveal.getQueryHash().transition || 'default'
            // Optional reveal.js plugins
            dependencies: [
                { src: '/ML_SA-Slides/lib/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({tabReplace: highlightjsTabSize}); hljs.initHighlightingOnLoad(); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/zoom-js/zoom.js', async: true },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/notes/notes.js', async: true }
            ]
        });
    </script>
    <!-- linkedin badge -->
    <!--<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async src="/ML_SA-Slides/lib/MathJax/MathJax.js?config=TeX-AMS_HTML-full"></script>
</body>
</html>
