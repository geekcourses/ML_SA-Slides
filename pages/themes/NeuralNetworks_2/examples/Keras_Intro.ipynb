{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Keras\n",
    "\n",
    "## The task:\n",
    "In next example we will create a simple binary classifier that predicts whether a person is an adult or a child based on their height and weight\n",
    "\n",
    "## Simple Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 17:36:42.701225: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 17:36:42.898851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:36:42.898894: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-02 17:36:44.550612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:36:44.550709: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:36:44.550719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some dummy train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181.07913344  68.73341811] => 1\n",
      "[106.42928193  40.11185327] => 0\n",
      "[113.35962781  42.77172265] => 1\n",
      "[106.25980123  29.30382761] => 0\n",
      "[171.36351824  64.37737522] => 0\n"
     ]
    }
   ],
   "source": [
    "data = np.random.random((1000, 2))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Generate random height and weight data for adults and children\n",
    "adult_heights = np.random.normal(loc=175, scale=10, size=1000)\n",
    "adult_weights = np.random.normal(loc=70, scale=5, size=1000)\n",
    "child_heights = np.random.normal(loc=120, scale=10, size=1000)\n",
    "child_weights = np.random.normal(loc=30, scale=5, size=1000)\n",
    "\n",
    "# Combine the height and weight data and create labels for adults and children\n",
    "heights = np.concatenate((adult_heights, child_heights))\n",
    "weights = np.concatenate((adult_weights, child_weights))\n",
    "adult_labels = np.ones(1000, dtype=int)\n",
    "child_labels = np.zeros(1000, dtype=int)\n",
    "labels = np.concatenate((adult_labels, child_labels))\n",
    "\n",
    "# Shuffle and join the data\n",
    "indices = np.arange(len(labels))\n",
    "np.random.shuffle(indices)\n",
    "data = np.stack((heights[indices], weights[indices]), axis=-1)\n",
    "labels = labels[indices]\n",
    "\n",
    "# print data with labels\n",
    "for i in range(5):\n",
    "    print(f'{data[i+300,:]} => {labels[i+100]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture\n",
    "\n",
    "We will create a neural network model with two sequential dense layers (a layer of neurons in which each neuron is connected to every neuron in the preceding layer):\n",
    "\n",
    "1. The first one will be the input layer with 2 units (neurons) and uses the relu activation function. It also has an input dimentions of 2, which specifies that the input data to the neural network has two features.\n",
    "\n",
    "2. The second dense layer - the output one - has 1 unit and uses the sigmoid activation function. This layer is used for binary classification problems, where the output is a single probability value between 0 and 1 indicating the probability of the input belonging to the positive class.\n",
    "\n",
    "With Sequential() function, we create a model object to which we add layers, one at a time, using the .add() method. This is a simple and straightforward way to create a neural network in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 17:49:34.354451: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:49:34.355100: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-02 17:49:34.355151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (x1c5): /proc/driver/nvidia/version does not exist\n",
      "2023-03-02 17:49:34.357003: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=2, activation='relu', input_dim=2)) # input layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\t\t\t   # output layer\t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to select the optimal number of units in a layer?\n",
    "\n",
    "The optimal number of units for a given layer depends on several factors, including the complexity of the problem, the size of the training data, and the architecture of the neural network. \n",
    "Here are some general guidelines that you can follow to set the number of units in a layer:\n",
    "\n",
    "*Start with a small number of units*: It's usually a good idea to start with a small number of units and gradually increase the number as needed. This can help prevent overfitting and reduce the computational cost of training the network.\n",
    "\n",
    "*Use a rule of thumb*: There are several rules of thumb for determining the number of units in a layer, such as the \"sqrt\" rule, which suggests using the square root of the number of inputs to the layer (i.e. number of features), or the \"2/3\" rule, which suggests using 2/3 of the number of inputs to the layer. These rules can provide a starting point for setting the number of units, but they may not be optimal for all problems.\n",
    "\n",
    "Experiment with different values: The best way to determine the optimal number of units for a given layer is to experiment with different values and evaluate the performance of the network on a validation set. You can try increasing or decreasing the number of units in the layer and see how it affects the performance of the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 1s 5ms/step - loss: 2.3361 - accuracy: 0.4990\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4950\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4970\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3985339a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, labels, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "5 test cases PASSED\n",
      "5 test cases FAILED\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    # Define 10 test cases\n",
    "    test_cases = [\n",
    "        {'input': [170, 60], 'expected_output': 1},\n",
    "        {'input': [140, 40], 'expected_output': 0},\n",
    "        {'input': [150, 50], 'expected_output': 1},\n",
    "        {'input': [130, 60], 'expected_output': 0},\n",
    "        {'input': [160, 70], 'expected_output': 1},\n",
    "        {'input': [120, 30], 'expected_output': 0},\n",
    "        {'input': [180, 80], 'expected_output': 1},\n",
    "        {'input': [140, 50], 'expected_output': 0},\n",
    "        {'input': [160, 50], 'expected_output': 0},\n",
    "        {'input': [150, 70], 'expected_output': 1}\n",
    "    ]\n",
    "    # Evaluate the model on the test cases\n",
    "    num_passed = 0\n",
    "    num_failed = 0\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        input_data = np.array(test_case['input']).reshape(1, 2)\n",
    "        expected_output = test_case['expected_output']\n",
    "        predicted_output = model.predict(input_data)[0][0].round()\n",
    "        if predicted_output == expected_output:\n",
    "            num_passed += 1\n",
    "            # print(f'Test case {i+1} PASSED')\n",
    "        else:\n",
    "            num_failed += 1\n",
    "            # print(f'Test case {i+1} FAILED')\n",
    "\n",
    "    # Print summary\n",
    "    print(f'{num_passed} test cases PASSED')\n",
    "    print(f'{num_failed} test cases FAILED')\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve model performance: tune model hyperparameters\n",
    "\n",
    "We see that model performance is not quite good. We can improve it by:\n",
    "\n",
    "1. Increasing the amount of training data: The more data you have, the better your model can learn to generalize to new examples. You could try generating more training data or finding additional real-world data to add to your training set.\n",
    "\n",
    "2. Improve the quality of the training data: Make sure that your training data is representative of the problem you're trying to solve and that it is labeled correctly. If your training data contains errors or is biased, your model's performance will suffer.\n",
    "\n",
    "3. Adjust the model architecture: Experiment with different numbers of layers, units, and activation functions in your model. You could also try adding regularization (such as dropout or weight decay) to prevent overfitting.\n",
    "\n",
    "4. Tune the hyperparameters: The hyperparameters (such as learning rate, batch size, and number of epochs) can have a big impact on your model's performance. Try different combinations of hyperparameters and use a validation set to evaluate their effectiveness.\n",
    "\n",
    "We can can use  [Scikit-Learn GridSearchCV function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to tune the hyperparameters of our model and KerasClassifier function that allows to use a Keras model as a Scikit-learn estimator.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n",
    "`GridSearchCV(estimator=model, param_grid=param_grid, cv=3)`\n",
    "\n",
    "where arguments are: \n",
    "\n",
    "estimator: This is the model to be trained and tuned. In this case, it is a Keras model that has been wrapped with the KerasClassifier function.\n",
    "\n",
    "param_grid: This is a dictionary or a list of dictionaries that defines the hyperparameters to be tuned and their possible values. Each key in the dictionary corresponds to a hyperparameter of the estimator, and each value is a list of possible values for that hyperparameter. For example, param_grid = {'num_units': [32, 64, 128], 'dropout_rate': [0.0, 0.2, 0.5], 'learning_rate': [0.01, 0.001, 0.0001]} defines a grid search over the hyperparameters num_units, dropout_rate, and learning_rate, with each hyperparameter taking on three possible values.\n",
    "\n",
    "cv: This is the number of folds in the cross-validation procedure. In this case, cv=3 specifies that a 3-fold cross-validation will be used.\n",
    "\n",
    "When GridSearchCV is executed with these arguments, it will perform a search over the parameter grid defined by param_grid using cross-validation with cv folds. It will fit the model for each combination of hyperparameters in the grid, and then evaluate the performance of each combination using cross-validation. Finally, it will return the best hyperparameters and their corresponding performance score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KerasClassifier.\n",
    "\n",
    "\n",
    "The KerasClassifier function is a wrapper function provided by the Keras library that allows you to use a Keras model as a Scikit-learn estimator. \n",
    "The KerasClassifier function takes as input a function that creates and returns a Keras model, and returns a Scikit-learn estimator that can be used in Scikit-learn pipelines, cross-validation, and grid search.\n",
    "\n",
    "`KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)`\n",
    "\n",
    "where arguments are:\n",
    "\n",
    "build_fn: This is a function that creates and returns a Keras model. The function should take no arguments and should return a compiled Keras model.\n",
    "\n",
    "epochs: This is the number of times the entire training dataset will be passed through the neural network during training.\n",
    "\n",
    "batch_size: This is the number of samples that will be propagated through the neural network at once during training.\n",
    "\n",
    "verbose: This controls the verbosity of the output during training. Setting verbose=0 means that no output will be printed during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87148/148004745.py:22: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: {'dropout_rate': 0.0, 'learning_rate': 0.01, 'num_units': 128}, Accuracy: 0.9634994864463806\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define a function to create the model with a given set of hyperparameters\n",
    "def create_model(learning_rate=0.1, num_units=2, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=num_units, activation='relu', input_dim=2))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier object with the create_model function and fit it to the training data\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "model.fit(data, labels)\n",
    "\n",
    "# Define a dictionary of hyperparameters and their values to test\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'num_units': [32, 64, 128],\n",
    "    'dropout_rate': [0.0, 0.2, 0.4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best set of hyperparameters\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(data, labels)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding accuracy\n",
    "print(f'Best: {grid_result.best_params_}, Accuracy: {grid_result.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3527530a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### create a new instance of your model with the optimal settings.\n",
    "\n",
    "# Define the best hyperparameters found by GridSearchCV\n",
    "best_hyperparams = {'dropout_rate': 0.0, 'learning_rate': 0.01, 'num_units': 128}\n",
    "\n",
    "# Create a new model instance with the best hyperparameters\n",
    "model = create_model(**best_hyperparams)\n",
    "\n",
    "# Train the model on your training data\n",
    "model.fit(data, labels, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "7 test cases PASSED\n",
      "3 test cases FAILED\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80d53e83da47a8dbd3f05a73e91e3e7dbf0e420a2da730c28d0eaee714054d51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
