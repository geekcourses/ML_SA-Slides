<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>ANN_Introduction</title>
    <link rel="shortcut icon" href="/ML_SA-Slides/favicon.ico">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <!-- css & themes include -->
    <link rel="stylesheet" href="/ML_SA-Slides/lib/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="/ML_SA-Slides/outfit/css/themes/projector.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? '/ML_SA-Slides/lib/reveal.js/css/print/pdf.css' : '/ML_SA-Slides/lib/reveal.js/css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
    <!-- CUSTOM -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
    <base target="_blank">
</head>
<body>
    <div class="reveal default center" data-transition-speed="default" data-background-transition="default">
    <div class="top_links">
        <a class="home_link" href="/ML_SA-Slides/pages/agenda/agenda.html#ANN_Introduction" target="_top"></a>
        <span class="help_link"><i class="fa-solid fa-circle-question"></i></span>
        <div class="help_text">
            <div class="note">Keyboard shortcuts:</div>
            <div><span>N/Спейс</span><span>Next Slide</span></div>
            <div><span>P</span><span>Previous Slide</span></div>
            <div><span>O</span><span>Slides Overview</span></div>
            <div><span>ctrl+left click</span><span>Zoom Element</span></div>
            <div class="print-howto"><br>If you want print version => add '<code>?print-pdf</code>' <br> at the end of slides URL (remove '#' fragment) and then print. <br>
            Like: https://wwwcourses.github.io/...CourseIntro.html?print-pdf </div>
        </div>
    </div>
    <div class="footer theme_switch">
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/dark.css'); return false;">Dark</a>
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/light.css'); return false;">Light</a>
        <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ML_SA-Slides/outfit/css/themes/projector.css'); return false;">Projector</a>
    </div>
    <div class="slides">
<!--
########################################################
##################### SLIDES START #####################
########################################################
-->
<section data-min-total="__X__" class="presentation-title"><h1>Introduction to Artificial Neural Networks (ANN)</h1></section>
<section data-transition="zoom">
    <section class="copyright" data-transition="zoom">
        <div>
            <p style="text-align: center;">Created for</p>
        </div>
        <div class="company">
            <a href="https://softwareacademy.bg/">
            <img style="height:80vh" src="/ML_SA-Slides/outfit/images/logos/software-web@4x.png" alt="software-web@4x.png">
            </a>
        </div>
        <div class="author">
            <span class="note"><a href="https://www.linkedin.com/in/ivapopova/">Iva E. Popova</a>,  2024,</span>
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
            <!-- <i class="fa fa-linkedin"></i> -->
        </div>
    </section>
</section>


<section data-min="__Y__" class="main-section-title"><h1>Overview</h1></section>
<section class="sub-sections"><h2>Overview</h2>
    <section>
        <dl class="fa">
            <dt>Artificial Neural Networks (ANNs) are a subfield of Machine Learning and AI that is inspired by the structure and function of the human brain.</dt>
            <dt>ANNs are composed of a large number of interconnected processing units called neurons, which are organized in layers.</dt>
            <dt>ANNs are used to model complex nonlinear relationships between input and output data and to <span class="note">learn from examples without being explicitly programmed</span>.</dt>
        </dl>
        <a href="./images/DL.gif"><img src="./images/DL.gif" alt="" style="height: 50vh;"></a>
    </section>
    <section><h3>The inspiration</h3>
        <dl class="fa">
            <dt>Artificial Neural Networks (ANNs) are inspired by the structure and function of the human brain. The idea of ANNs is based on the concept that the brain is a network of interconnected neurons that communicate with each other to process and transmit information. </dt>
            <dt>The neurons in the human brain receive input signals from other neurons through dendrites, perform computations in the cell body, and then produce an output signal through an axon that connects to other neurons. The communication between neurons in the brain occurs through synapses, which are the connections between the axon of one neuron and the dendrites of another neuron.</dt>
        </dl>
    </section>
    <section><h3>Biological Neuron vs Artificial Neuron</h3>
        <div style="display: flex;">
            <div>
                <a href="./images/bioneuron_vs_artneuron.webp"><img src="./images/bioneuron_vs_artneuron.webp" alt="bioneuron_vs_artneuron" style="height: 80vh;"></a>
            <span>source: <a href="https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7">https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7</a></span>
            </div>
            <table>
                <tr>
                    <th>Biological Neuron</th>
                    <th>Artificial Neuron</th>
                </tr>
                <tr>
                    <td>dendrites</td>
                    <td>input</td>
                </tr>
                <tr>
                    <td>soma (cell nucleus)</td>
                    <td>node</td>
                </tr>
                <tr>
                    <td>axon</td>
                    <td>output</td>
                </tr>
                <tr>
                    <td>synapses</td>
                    <td>interconnections</td>
                </tr>
            </table>
        </div>
    </section>
    <section><h3>The neuron</h3>
        <dl class="fa">
            <dt>The neurons in an ANN receive input signals, perform computations, and then produce an output signal.</dt>
            <dt>The input signals are usually multiplied by a set of weights, which are adjusted during the training process to minimize the error between the predicted output and the actual output. </dt>
            <dt>The weighted input signals are then passed through an activation function, which produces the output signal.</dt>
        </dl>
    </section>
</section>

<section data-min="50" class="main-section-title"><h1>ANN Structure</h1></section>
<section class="sub-sections"><h2>ANN Structure</h2>
    <section>
        <dl class="fa">
            <dt>The structure of an ANN can be described as a directed graph, where each neuron in one layer is connected to every neuron in the next layer. </dt>
            <dt>The input layer is where the data is fed into the network, and the output layer produces the result. </dt>
            <dt>In between, there can be one or more hidden layers, which are used to extract relevant features from the input data.</dt>
            <a href="./images/ANN_Structure.png"><img src="./images/ANN_Structure.png" alt="ANN_Structure.png"></a>
        </dl>
    </section>
    <section><h3>Layers</h3>
        <dl class="fa">
            <dt>Input layer</dt>
            <dd>This is the layer where the input data is fed into the network.</dd>
            <dd>The number of neurons in the input layer is determined by the size of the input data.</dd>
            <dd>For example, if we are using a neural network for image classification, the input layer will have neurons corresponding to the pixels of the image.</dd>
            <dt>Hidden layer</dt>
            <dd>This is the layer that lies between the input and output layers of the network.</dd>
            <dd>The number of hidden layers and the number of neurons in each hidden layer can vary depending on the complexity of the problem being solved</dd>
            <dd>The hidden layer(s) are used to extract relevant features from the input data</dd>
            <dt>Output layer</dt>
            <dd>This is the final layer of the network that produces the output. </dd>
            <dd>The number of neurons in the output layer is determined by the type of problem being solved. </dd>
            <dd>For example, in a binary classification problem, the output layer will have two neurons corresponding to the two possible classes.</dd>
        </dl>
    </section>
    <section id="DenseLayer"><h3>Dense Layer</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>A Dense layer, also known as a fully connected layer, is a fundamental building block in neural networks.</dt>
            <dt>In a Dense layer, each neuron is connected to every neuron in the previous layer. This means that every input to the Dense layer contributes to the output of each neuron in the layer.</dt>
            <dt>The output of a Dense layer is computed using the following formula:</dt>
            <dd>
                <em>output = activation(input ⋅ weights + bias)</em>
            </dd>
            <dd >Where:</dd>
            <ul style="margin-left: 4em">
                <li><strong>input</strong>: The data from the previous layer.</li>
                <li><strong>weights</strong>: The trainable parameters that define the importance of each input for each neuron.</li>
                <li><strong>bias</strong>: A trainable offset value added to the weighted sum.</li>
                <li><strong>activation</strong>: A function (e.g., ReLU, Sigmoid) that introduces non-linearity to the output.</li>
            </ul>

        </dl>
    </section>
    <section><h3>Example of a Dense Layer in Keras</h3>
        <dl class="fa" style="min-width:80vw">

            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                from keras.models import Sequential
                from keras.layers import Dense

                # Create a simple model with a Dense layer
                model = Sequential()
                # Add a Dense layer with 32 neurons and ReLU activation
                model.add(Dense(32, input_dim=8, activation='relu'))
            </code></pre>

            <dt>The layer contains 32 neurons. Each neuron receives input from every neuron in the previous layer (or the input data).</dt>
            <dt>The layer expects an input of 8 features. This means the input to the network has 8 dimensions or attributes, which are passed to each of the 32 neurons.</dt>
            <dt>Each of the 32 neurons has 8 corresponding weights (one for each input feature) and a bias term.</dt>
            <dd>The total number of trainable parameters for this layer is \( (8 \times 32) + 32 = 288 \) (weights + biases).</dd>
            <dt>The ReLU (Rectified Linear Unit) activation function is applied to the output of each neuron. This introduces non-linearity, making the network capable of learning complex patterns.</dt>
        </dl>
    </section>
</section>

<section data-min="50" class="main-section-title"><h1>Activation Function</h1></section>
<section class="sub-sections"><h2>Activation Function</h2>
    <section>
        <dl class="fa">
            <dt>Activation functions are an essential part of Neural Networks as they are responsible for introducing non-linearity into the model. Without activation functions, Neural Networks would essentially be a series of linear operations, which would severely limit the model's capacity to learn complex patterns.</dt>
            <dt>There are several types of activation functions used in ANNs, including sigmoid, ReLU, and tanh. </dt>
            <dt>The choice of activation function depends on the problem being solved and the type of data being processed.</dt>
        </dl>
    </section>
    <section><h3>Sigmoid Function</h3>
        <dl class="fa">
            <dt>
                The Sigmoid function is one of the earliest and most widely used activation functions in Neural Networks. The Sigmoid function maps any input value to a value between 0 and 1, which makes it useful in binary classification problems. The mathematical formula for the Sigmoid function is given below:

                $$\\sigma(x) = \frac{1}{1 + e^{-x}}$$

            </dt>
            <dt>The Sigmoid function has the following properties:</dt>
            <dd>It is a smooth, continuous function.</dd>
            <dd>Its output is always between 0 and 1.</dd>
            <dd>The output is centered around 0.5, which can make it problematic in cases where the input values are very large or very small.</dd>
        </dl>
        <img src="./images/sigmoit-plot.png" alt="">
    </section>
    <section><h3>ReLU Function</h3>
        <dl class="fa">
            <dt>The Rectified Linear Unit (ReLU) function is another popular activation function used in Neural Networks. The ReLU function maps any input value to either 0 or the input value itself, which makes it useful in cases where we want to introduce sparsity in the model. The mathematical formula for the ReLU function is given below:

                $$ReLU(x) = max(0, x)$$</dt>
            <dt>The ReLU function has the following properties:</dt>
            <dd>It is a simple, non-linear function.</dd>
            <dd>It is computationally efficient to compute.</dd>
            <dd>The output is sparse, which can help with overfitting.</dd>
        </dl>

        <img src="./images/ReLU-plot.png" alt="">
    </section>
    <section><h3>Tanh Function</h3>
        <dl class="fa">
            <dt>The Tanh function is another popular activation function that maps any input value to a value between -1 and 1. The Tanh function is useful in cases where we want to introduce non-linearity and can help with normalization of the data. The mathematical formula for the Tanh function is given below:

                $$tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$
            </dt>
            <dt>The Tanh function has the following properties:</dt>
            <dd>It is a smooth, continuous function.</dd>
            <dd>Its output is always between -1 and 1.</dd>
            <dd>The output is centered around 0, which can help with normalization of the data.</dd>
        </dl>
        <img src="./images/Tanh-plot.png" alt="">
    </section>
    <!-- <section><h3>Softmax Function</h3>
        <dl class="fa">
            <dt>
                The Softmax function is commonly used in the output layer of Neural Networks that are designed for multi-class classification tasks. The Softmax function maps a vector of real-valued numbers to a probability distribution over the classes. The mathematical formula for the Softmax function is given below:

                $$softmax(x\_i) = \frac{e^{x\_i}}{\\sum\_{j=1}^{K} e^{x\_j}}$$

                where $K$ is the number of classes.
            </dt>
            <dt>
                The Softmax function has the following properties:</dt>
                <dd>It produces a probability distribution over the classes.</dd>
                <dd>The output values are always between 0 and 1 and sum up to 1.</dd>
        </dl>
        <img src="./images/softmax-plot.png" alt="softmax">
    </section> -->
</section>

<section data-min="50" class="main-section-title"><h1>ANN training</h1></section>
<section class="sub-sections"><h2>ANN training. Backpropagation.</h2>
    <section>
        <dl class="fa">
            <dt>Training an ANN involves adjusting the weights of the connections between neurons to minimize the error between the predicted output and the actual output. </dt>
            <dt>This is done using a process called backpropagation, which involves calculating the gradient of the error with respect to each weight and bias in the network, and using this gradient to update the values of the weights and biases in the direction that reduces the error.</dt>
        </dl>
    </section>
    <section><h3>Backpropagation Algorithm</h3>
        <p>Here's how it works in simple terms:</p>
        <dl class="fa">
            <dt>1. Feedforward: The input data is fed into the network, and the output is calculated by passing it through a series of interconnected nodes, also known as neurons.</dt>
            <dt>2. Calculate Error: The difference between the predicted output and the actual output is calculated, and this is known as the error.</dt>
            <dt>3. Backpropagation: The error is then propagated backwards through the network to adjust the weights and biases. The weights and biases are updated in the direction that reduces the error, using an optimization algorithm such as Stochastic Gradient Descent.</dt>
            <dt>4. Repeat: Steps 1-3 are repeated for each example in the training dataset until the error is minimized.</dt>
        </dl>
    </section>
</section>

<section data-min="50" class="main-section-title"><h1>Perceptron</h1></section>
<section class="sub-sections"><h2>Perceptron</h2>
    <section><h3>Overview</h3>
        <dl class="fa">
            <dt>A perceptron is a type of artificial neural network that is used for binary classification problems. It is the simplest type of neural network and consists of a single layer of neurons that are fully connected to the input data.</dt>
            <dt>The perceptron algorithm was introduced by Frank Rosenblatt in the 1950s and is based on the concept of a biological neuron.</dt>
            <dt>A single perceptron can only be used to implement linearly separable functions</dt>
        </dl>
    </section>
    <section>
        <dl class="fa">
            <a href="./images/Perceptron.png"><img src="./images/Perceptron.png" alt="Perceptron.png"></a>
            <dt>The input to a perceptron is a vector of real-valued numbers that represents the features of the input data: <code>in(t)</code></dt>
            <dt>Each feature ($x_i$) is associated with a weight ($w_i$), which is a real-valued number that determines the importance of the feature.</dt>
            <dt>The weighted sum of the inputs is then passed through an activation function, which produces the output of the perceptron.</dt>
            <dt>The activation function used in a perceptron is a step function that produces a <span class="note">binary output</span>.</dt>
        </dl>
    </section>
    <section><h3>Perceptron Activation Function</h3>
        <dl class="fa">
            <dt>If the weighted sum of the inputs is greater than a threshold value ($\theta$), the perceptron outputs a 1; otherwise, it outputs a 0:
                $$
                \text{out(t)} = \begin{cases}
                1, &\text{if } w_1x_1 + w_2x_2 + \dots + w_nx_n \geq \theta \\
                0, &\text{if } w_1x_1 + w_2x_2 + \dots + w_nx_n \lt \theta
                \end{cases}
                $$
                <br>
                where $w_1$, $w_2$, ..., $w_n$ are the weights associated with the input features $x_1$, $x_2$, ..., $x_n$.
                <br>
            </dt>
            <dt>To make things a little simpler for training, the threshold is moved to the other side of the inequality:
                $$
                \text{out(t)} = \begin{cases}
                1, &\text{if } w_1x_1 + w_2x_2 + \dots + w_nx_n + bias \geq 0 \\
                0, &\text{if } w_1x_1 + w_2x_2 + \dots + w_nx_n + bias \lt 0
                \end{cases}
                $$

                where bias = -threshold
            </dt>

        </dl>
    </section>
    <section><h3>Example: OR Function Using A Perceptron</h3>
        <dl class="fa">
            <dt><a href="https://github.com/geekcourses/ML_SA-Slides/blob/main/pages/themes/ANN_Introduction/examples/Perceptron_OR_Function.ipynb">Perceptron_OR_Function</a></dt>
        </dl>
    </section>
    <section id="howToImplementPerceptronFromScratchWithPython"><h3>How to implement Perceptron from scratch with Python</h3>
        <iframe width="1280" height="720" src="https://www.youtube.com/embed/aOEoxyA4uXU" title="How to implement Perceptron from scratch with Python" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </section>
</section>

<section data-min="50" class="main-section-title"><h1>ANN Types</h1></section>
<section class="sub-sections"><h2>ANN Types</h2>
    <section><h3>Feedforward Neural Networks (FNNs)</h3>
        <dl class="fa">
            <dt>This is the simplest type of neural network, where the data flows only in one direction, from the input layer to the output layer. </dt>
            <dt>FNNs can have one or more hidden layers. </dt>
            <dt>The neurons in the input layer are connected to the neurons in the hidden layer, and the neurons in the hidden layer are connected to the neurons in the output layer.</dt>
        </dl>
    </section>
    <section><h3>Convolutional Neural Networks (CNNs)</h3>
        <dl class="fa">
            <dt>CNNs are used for image and video recognition tasks.</dt>
            <dt>They use a convolutional layer to extract relevant features from the input image, followed by a pooling layer to reduce the dimensionality of the output.</dt>
            <dt>The output of the pooling layer is then fed into a fully connected layer to produce the final output.</dt>
        </dl>
    </section>
    <section><h3>Recurrent Neural Networks (RNNs)</h3>
        <dl class="fa">
            <dt>RNNs are used for sequential data processing tasks, such as speech recognition and natural language processing. </dt>
            <dt>RNNs have a feedback loop that allows the output of a neuron to be fed back into the input of the same neuron or other neurons in the network. </dt>
            <dt>This allows RNNs to model the temporal dependencies in sequential data.</dt>
        </dl>
    </section>
    <section><h3>Long Short-Term Memory Networks (LSTMs)</h3>
        <dl class="fa">
            <dt>LSTMs are a type of RNN that are designed to address the vanishing gradient problem. </dt>
            <dt>LSTMs use memory cells that can store information for long periods of time, allowing the network to learn long-term dependencies in sequential data.</dt>
        </dl>
    </section>
    <section><h3>References</h3>
        <dl class="fa">
            <dt><a href="https://www.asimovinstitute.org/neural-network-zoo/">The neural network zoo</a></dt>
        </dl>
    </section>
</section>

<section class="main-section-title" id="ProsAndConsOfANN"><h1>Pros and Cons of Artificial Neural Networks</h1></section>
<section class="sub-sections"><h2>Pros and Cons of Artificial Neural Networks</h2>
    <section id="ProsOfANN"><h3>Pros of Artificial Neural Networks</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>1. <strong>Ability to Learn Nonlinear Relationships:</strong></dt>
            <dd>ANNs can model complex nonlinear relationships between inputs and outputs, making them effective for tasks such as image recognition, speech processing, and more.</dd>

            <dt>2. <strong>Adaptability:</strong></dt>
            <dd>ANNs can be trained to solve a variety of problems without needing specific rules, allowing them to generalize to new data once trained.</dd>

            <dt>3. <strong>Scalability:</strong></dt>
            <dd>ANNs can scale from simple networks to deep architectures, allowing them to be used in small to large-scale applications (e.g., simple classification to deep learning tasks like NLP).</dd>
        </dl>
    </section>
    <section id="ConsOfANN"><h3>Cons of Artificial Neural Networks</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>1. <strong>Computationally Intensive:</strong></dt>
            <dd>Training ANNs, especially deep networks, requires significant computational resources, often involving GPUs or specialized hardware.</dd>

            <dt>2. <strong>Data Dependency:</strong></dt>
            <dd>ANNs require large datasets to achieve good performance. Without sufficient data, they are prone to overfitting or underfitting.</dd>

            <dt>3. <strong>Black Box Nature:</strong></dt>
            <dd>It is difficult to interpret the inner workings of ANNs, making it hard to explain why certain decisions are made, which can be a drawback in critical applications like healthcare or finance.</dd>
        </dl>
    </section>
</section>


<section class="main-section-title" id="ApplicationsOfANN"><h1>Applications of Artificial Neural Networks</h1></section>
<section class="sub-sections"><h2>Applications of Artificial Neural Networks</h2>
    <section id="LearningTasksForANN"><h3>Learning Tasks for Artificial Neural Networks</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Artificial Neural Networks (ANNs) are applied in various types of learning tasks:</dt>

            <dt>Supervised Learning</dt>
            <dd>ANNs are widely used for tasks like classification, regression, and predictive modeling, where labeled data is available (e.g., image recognition, speech recognition).</dd>

            <dt>Unsupervised Learning</dt>
            <dd>ANNs are employed in tasks like clustering, dimensionality reduction, and anomaly detection, typically using techniques like Autoencoders and Self-Organizing Maps (SOM).</dd>

            <dt>Reinforcement Learning</dt>
            <dd>ANNs are integrated into reinforcement learning for decision-making tasks, particularly in environments where agents learn from rewards (e.g., Deep Q-Networks, AlphaGo).</dd>
        </dl>
    </section>
    <section id="ImageRecognition"><h3>Image Recognition</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Convolutional Neural Networks (CNNs), a type of ANN, are widely used for image classification and object detection tasks.</dt>
            <dd>They have shown remarkable success in recognizing objects in photos, medical images, and facial recognition systems.</dd>
            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                # Example of loading a pre-trained CNN for image recognition
                from tensorflow.keras.applications import VGG16

                model = VGG16(weights='imagenet')
                # Model can now be used for image classification
            </code></pre>
        </dl>
    </section>

    <section id="NaturalLanguageProcessing"><h3>Natural Language Processing</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Recurrent Neural Networks (RNNs) and Transformer models are widely used in NLP tasks.</dt>
            <dd>Applications include language translation, sentiment analysis, and chatbots like GPT, which are based on Transformer architecture.</dd>
            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                # Example of using GPT-3 for text generation
                import openai

                openai.api_key = 'your-api-key'
                response = openai.Completion.create(
                    model="text-davinci-003",
                    prompt="Explain ANN in simple terms.",
                    max_tokens=50
                )
                print(response.choices[0].text.strip())
            </code></pre>
        </dl>
    </section>

    <section id="GameAI"><h3>Game AI</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>ANNs are used to train intelligent agents in games to make decisions, strategize, and even learn from the game environment.</dt>
            <dt>Deep Reinforcement Learning (which uses Deep Neural Networks) has been applied to create AI that can outperform humans in games like Go and Dota 2.</dt>
        </dl>
    </section>
</section>

<section class="main-section-title" id="UsingPythonLibrariesForANN"><h1>Using Python Libraries for Artificial Neural Networks</h1></section>
<section class="sub-sections"><h2>Using Python Libraries for Artificial Neural Networks</h2>
    <section id="TensorFlow"><h3>TensorFlow</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>TensorFlow is an open-source library developed by Google, widely used for building and training machine learning models, especially deep learning models.</dt>
            <dd>TensorFlow is highly scalable and can be deployed in both research and production environments. It supports both low-level operations and high-level APIs for neural networks.</dd>
        </dl>
    </section>
    <section id="Keras"><h3>Keras</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Keras is a high-level API built on top of TensorFlow that simplifies the process of creating and training deep learning models.</dt>
            <dd>Keras is known for its user-friendliness, ease of prototyping, and flexibility in building both simple and complex models.</dd>
        </dl>
    </section>
    <section id="PyTorch"><h3>PyTorch</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>PyTorch is an open-source machine learning library developed by Facebook, widely used for research and prototyping.</dt>
            <dd>PyTorch provides dynamic computational graphs, which allow for easier debugging and experimentation. It is also known for its flexibility and ease of use in creating complex neural network architectures.</dd>
        </dl>
    </section>

</section>

<section class="main-section-title" id="ExampleUsingKeras"><h1>Example: Creating an ANN with Keras</h1></section>
<section class="sub-sections"><h2>Example: Creating an ANN with Keras</h2>
    <section id="KerasExample"><h3>Keras Example</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The network consists of two layers:</dt>
            <dd></dd>
                1. <strong>Input Layer and Hidden Layer:</strong> The first layer is a Dense (fully connected) layer with 32 neurons and the ReLU activation function. It accepts an input of 8 dimensions (features).
            </dd>
            <dd>
                2. <strong>Output Layer:</strong> The second layer is the output layer with 1 neuron, using the sigmoid activation function. This is ideal for binary classification tasks, as it produces an output between 0 and 1.
            </dd>
            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                # Import necessary libraries
                from keras.models import Sequential
                from keras.layers import Dense

                # Create a simple ANN
                model = Sequential()
                model.add(Dense(32, input_dim=8, activation='relu'))  # Input layer and first hidden layer
                model.add(Dense(1, activation='sigmoid'))  # Output layer

                # Compile the model
                model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

                # Train the model on the training data (X_train, y_train)
                model.fit(X_train, y_train, epochs=10, batch_size=32)
            </code></pre>
        </dl>
    </section>
</section>

<section class="main-section-title" id="ExampleUsingPyTorch"><h1>Example: Creating an ANN with PyTorch</h1></section>
<section class="sub-sections"><h2>Example: Creating an ANN with PyTorch</h2>
    <section id="PyTorchExample"><h3>PyTorch Example</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The network consists of two layers:</dt>
            <dd>
                1. <strong>Input Layer and Hidden Layer:</strong> The first layer is a fully connected layer with 32 neurons. It uses the ReLU activation function and takes 8 input features.
            </dd>
            <dd>
                2. <strong>Output Layer:</strong> The second layer is the output layer with 1 neuron, using the sigmoid activation function. This is designed for binary classification tasks.
            </dd>
            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                import torch
                import torch.nn as nn
                import torch.optim as optim

                # Define a simple ANN
                class SimpleANN(nn.Module):
                    def __init__(self):
                        super(SimpleANN, self).__init__()
                        self.fc1 = nn.Linear(8, 32)
                        self.fc2 = nn.Linear(32, 1)

                    def forward(self, x):
                        x = torch.relu(self.fc1(x))
                        x = torch.sigmoid(self.fc2(x))
                        return x

                # Initialize the model, loss function, and optimizer
                model = SimpleANN()
                criterion = nn.BCELoss()  # Binary cross-entropy loss
                optimizer = optim.Adam(model.parameters())

                # Training loop (assuming X_train and y_train are available)
                for epoch in range(10):
                    optimizer.zero_grad()
                    outputs = model(X_train)
                    loss = criterion(outputs, y_train)
                    loss.backward()
                    optimizer.step()
            </code></pre>
        </dl>
    </section>
</section>



<!-- <section data-min="1"><h1>References</h1></section>
<section><h2>References</h2>
    <section><h3>Readings</h3>
        <dl class="fa">
            <dt></dt>
        </dl>
    </section>
</section>


<section id="hw" data-min="4"><h1>Exercises</h1></section>
<section><h2>Task1: Task1Title</h2>
    <section><h3>The Task</h3>
        <dl class="fa">
            <dt></dt>
        </dl>
    </section>
</section>

<section><h3>Submission</h3>
    <dl class="fa">
        <dt>PLease, prefix your filenames/archive with your name initials, before sending.</dt>
        <dd>For instance: <b>iep_task1.py</b> or <b>iep_tasks.rar</b></dd>
        <dt>Send files to <a href="mailto:netIT.WWW.Courses@gmail.com?Subject=_ANN_Introduction_">netIT.WWW.Courses@gmail.com</a></dt>
    </dl>
</section> -->

<section class="disclaimer end-slide"></section>
<!--
########################################################
##################### SLIDES END   #####################
########################################################
-->
        </div>
    </div>
    <!-- Custom processing -->
    <script src="/ML_SA-Slides/outfit/js/slides.js"></script>
    <!-- external scripts -->
    <script src="/ML_SA-Slides/lib/reveal.js/lib/js/head.min.js"></script>
    <script src="/ML_SA-Slides/lib/reveal.js/js/reveal.js"></script>

    <!-- init reveal -->
    <script>
        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        var highlightjsTabSize = '  ';
        Reveal.initialize({
            controls: true,
            progress: true,
            slideNumber: 'c/t',
            keyboard: true,
            history: true,

            // display control
            // center: true,
            // width: '100%',
            // height: '100%',
            // // // Factor of the display size that should remain empty around the content
            // margin: 0.1,

            // The "normal" size of the presentation, aspect ratio will be preserved
            // when the presentation is scaled to fit different resolutions. Can be
            // specified using percentage units.
            width: 1920,
            height: 1280,

            // Factor of the display size that should remain empty around the content
            margin: 0.1,

            // Bounds for smallest/largest possible scale to apply to content
            minScale: 0,
            maxScale: 2,

            // slide transition
            transition: 'concave', // none/fade/slide/convex/concave/zoom
            // shift+maous click to zoom in/out element
            zoomKey: 'ctrl',
            // theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
            // transition: Reveal.getQueryHash().transition || 'default'
            // Optional reveal.js plugins
            dependencies: [
                { src: '/ML_SA-Slides/lib/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({tabReplace: highlightjsTabSize}); hljs.initHighlightingOnLoad(); } },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/zoom-js/zoom.js', async: true },
                { src: '/ML_SA-Slides/lib/reveal.js/plugin/notes/notes.js', async: true }
            ]
        });
    </script>
    <!-- linkedin badge -->
    <!--<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>-->

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async src="/ML_SA-Slides/lib/MathJax/MathJax.js?config=TeX-AMS_HTML-full"></script>
</body>
</html>
